{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "934843a6",
   "metadata": {},
   "source": [
    "![cropped-SummerWorkshop_Header.png](../../resources/cropped-SummerWorkshop_Header.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1376fe7c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h1 align=\"center\"> Neural Encoding </h1> \n",
    "<h2 align=\"center\"> SWDB 2024 - Day 2 - Morning Session </h2> \n",
    "<h3 align=\"center\"> Tuesday, August 20, 2024</h3> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48435f7d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h2>Neural Encoding </h2>\n",
    "\n",
    "Neural coding describes how neurons represent information about the world. Coding can be studied by asking whether external or internal events lead to changes in neural activity (<b>encoding</b>), or by asking whether different types of information can be read out from neural activity (<b>decoding</b>). In this workshop we will address the question of neural encoding, with a focus on single cell encoding models. \n",
    "\n",
    "Encoding of <b>sensory</b> information has been studied for decades by presenting animals with stimuli and observing how the activity of individual neurons changes. To study encoding of <b>motor</b> variables, researchers train animals to perform a behavior (or observe naturalistic behaviors) and correlate the animals' movement with changes in neural activity. Recent research has demonstrated that even sensory areas have representations of motor and behavioral variables, and vice versa. This is often called \"multiplexed\" coding. Furthermore, neural encoding can be influenced by <b>cognitive</b> processes such as learning, task engagement, and decision making. \n",
    "\n",
    "The exact form of neural activity changes is also a part of the study of neural coding. Neurons can represent information based on their average firing rates over a period of time, based on the precise spike times relative to some event, using bursts of spikes, or based on synchrony and timing relative to global population activity. In this workshop, we will consider <b>average firing rates</b> as they relate to information coding.\n",
    "\n",
    "To learn more, check out this lecture Principles of Neural Coding by I. Memming Park: https://www.youtube.com/watch?v=DlFxUEdGlmQ\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "115346a3",
   "metadata": {},
   "source": [
    "![neural_coding.png](../../resources/neural_coding.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88fe5849",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> \n",
    "\n",
    "Today we will look at how neurons in the visual cortex encode sensory and behavioral information in mice performing a visually guided task.\n",
    "\n",
    "The first workshop of the day will focus on how single cells encode information in their average activity patterns (the problem of X-->R in the schemtic above). In the afternoon, we will learn about how to decode information from populations of neurons (R-->X as shown above) and how variability and correlations influence decoding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47a41123",
   "metadata": {},
   "source": [
    "## Workshop Outline & Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06d9d1b5",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h3> What we will investigate today </h3>\n",
    "\n",
    "<h4> Part 1 -  Visual Behavior Ophys Dataset</h4>\n",
    "\n",
    "(1) What is the experimental design? What are the key considerations for the question of neural encoding?\n",
    "\n",
    "(2) What recording modality was used? Why choose this modality? What are the pros & cons?\n",
    "\n",
    "(3) How can we access the data we are interested in?\n",
    "\n",
    "\n",
    "<h4> Part 2 -  Tuning for stimulus & behavior during task performance </h4>\n",
    "\n",
    "(1) Can we find neurons in the mouse visual cortex that are selective for specific visual stimuli? How reliable are their responses?\n",
    "\n",
    "(2) Do stimulus responses differ depending on the mouse's behavioral choice during the task? \n",
    "\n",
    "(3) Do neurons in the mouse visual cortex modulate their activity as a function of running speed? \n",
    "\n",
    "\n",
    "<h4> Part 3 -  Quantifying single cell coding with regression models </h4>\n",
    "\n",
    "(1) How can linear regression be used to model neural coding? \n",
    "\n",
    "(2) How do you ensure that your model is valid and is not overfitting?\n",
    "\n",
    "(3) How well can you predict neural activity based on stimulus information? Behavioral information? \n",
    "\n",
    "(4) Does the prediction improve when additional variables are included? (multiple linear regression)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27862c9a",
   "metadata": {},
   "source": [
    "## Part 1 - Visual Behavior Ophys Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "275f98db",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3> Experimental Design </h3>\n",
    "\n",
    "(1) What is the experimental design? What are the key considerations for the question of neural encoding?\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03b0a4b7",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Stimulus / behavior paradigm: A visual change detection task </h4>\n",
    "\n",
    "<p> To understand how neurons encode sensory and behavioral variables, it is useful to examine neural activity under conditions where sensory input and behavioral outputs vary across conditions and show rich and interesting structure that can be used to disentangle their unique contributions to neural signals. \n",
    "\n",
    "In the <b>Visual Behavior Ophys</b> dataset, neural activity was recorded while mice performed a visual change detection task. Mice were presented with stimuli and asked to make choices about those stimuli, while their <b>running</b> and <b>licking</b> behavior were measured, along with <b>pupil diameter</b> as a proxy for the animals' internal state. \n",
    "\n",
    "During the change detection task, mice view a continuous stream of <b>natural scene images</b>, which are displayed for 250ms, followed by a 500ms gray screen period. The gray screen period adds a working memory component to the task. The job of the mouse is to decide - \"is what i am seeing now the same or different than what i saw 500ms ago?\". \n",
    "\n",
    "If the mouse correctly detects a change and reports their choice by licking a reward spout within 750ms of the change, the trial is considered a <b>hit</b>, and a water <b>reward</b> is delivered. If mice fail to lick after a change, the trial is a <b>miss</b>. If the mouse licks anytime outside of the reward window just after the change (called a false alarm or aborted trial), the trial resets and the mouse will have to wait longer until the next opportunity for a reward. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14ae7f4e",
   "metadata": {},
   "source": [
    "![change_detection_task.png](../../resources/change_detection_task.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c57569b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Opportunities to study encoding of stimulus & behavior variables </h4>\n",
    "\n",
    "<p> During the task, mice are free to run on a circular disc. Many studies have shown that locomotion can influence sensory signals and neurons can be tuned for running speed. \n",
    "Pupil diameter is also recorded, which can be used as a measure of overall behavioral state or arousal. When animals attend to a stimulus or are otherwise alert and active, the pupil dilates.\n",
    "In contrast, during quiet or inattentive states, the pupil constricts. Neural coding is also influenced by arousal state, as measured by pupil diameter. \n",
    "\n",
    "Thus, in this dataset, we can ask about encoding of: \n",
    "* <b>Sensory stimuli</b> - via the images that are presented to the mouse\n",
    "* <b> Behavioral choice</b>  - whether the mouse licks or not following a given stimulus presentation\n",
    "* <b> Rewards</b>  - which are given depending on whether or not the mouse made a correct choice\n",
    "* <b> Locomotion & arousal</b>  - via changes in animal running speed or pupil diameter\n",
    "\n",
    "There may be additional dimensions of sensory or behavioral events that are of interest - can you think of any? \n",
    "\n",
    "Some other examples could be the number of exposures to a give image after a change, or the time since the last reward received by the mouse, or past trial outcomes, or perhaps a combination of running and pupil together is informative about cell activity.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69420065",
   "metadata": {},
   "source": [
    "![behavior_timeseries_color.png](../../resources/behavior_timeseries_color.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d983056",
   "metadata": {},
   "source": [
    "There are some interesting dynamics here - how might they influence neural activity?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c52c081",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3> Recording method </h3>\n",
    "\n",
    "(2) What recording modality was used? Why choose this modality? What are the pros & cons?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7711396b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> 2-photon calcium imaging - pros & cons</h4>\n",
    "<p>Today we will use the Allen Brain Observatory Visual Behavior Ophys dataset. \n",
    "\n",
    "\"Ophys\" stands for Optical Physiology, and typically refers to 2-photon calcium imaging of cellular activity (although there are other optical physiology methods!). You can learn more about this method in the *<b>DataBook</b>*\n",
    "\n",
    "The critical thing to know for this workshop is that neural activity is measured as changes in the fluorescence of a genetically encoded calcium indicator, called GCaMP. Calcium influx occurs when neurons fire action potentials, thus the fluorescence of the calcium indicator is a proxy for neuronal spiking (however it is not a direct readout of neural spiking). \n",
    "\n",
    "Here are some additional considerations when choosing 2-photon calcium imaging as a recording modality:\n",
    "\n",
    "Pros:\n",
    "* Visualizing neurons in space\n",
    "* Genetic targeting of specific neuron types\n",
    "* Tracking neurons across multiple recording sessions\n",
    "\n",
    "Cons:\n",
    "* Slow acquisition rates relative to spike timing\n",
    "* Limited to 500um depth, typically in cortex\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c76f59",
   "metadata": {},
   "source": [
    "Here is an example of what 2-photon calcium imaging looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7cce2",
   "metadata": {},
   "source": [
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/WGhMynyympg?si=panDVPhaaSTUAAy_\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "732bb610",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Trangenic lines to label specific cell populations</h4>\n",
    "<p>\n",
    "In the <b>Visual Behavior Ophys</b> dataset, 3 different <b>transgenic mouse lines</b> were used to express GCaMP in either <b>excitatory neurons</b> (labeled by the Slc17a7-IRES2-Cre driver line), \n",
    "or in one of two types of <b>inhibitory neurons</b> - somatostatin (Sst) expressing neurons or vasoactive intestinal peptide (Vip) expressing neurons (Sst-IRES-Cre driver line and Vip-IRES-Cre driver lines, respectively). The reporter line driving GCaMP expression under the control of the Cre driver was either Ai93 or Ai148, both expressing GCaMP6f, which has fast kinetics. \n",
    "\n",
    "You can read more about transgenic mice in the <b>*DataBook*</b>\n",
    "\n",
    "Sst and Vip inhibitory neurons are known to mutually inhibit each other and a shift in the balance between them can lead to disinhibition of excitatory neurons under certain conditions. Sst and Vip inhibitory neurons in the visual cortex, along with excitatory cells, are known to show modulation by locomotion, arousal, attention, and learning. \n",
    "\n",
    "Here is a useful review on how animal behavior and learning influence the coding of different cell types in the visual cortex: \n",
    "https://doi.org/10.1016/j.conb.2018.04.020\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea98ee",
   "metadata": {},
   "source": [
    "![cre_lines.png](../../resources/cre_lines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0a8d3",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "This dataset will allow us to ask not only about how neurons encode information, but also to ask which types of neurons encode which types of information. \n",
    "Combined with knowledge about anatomy and connectivity, this can help us understand how the brain computes information using circuits built up of unique cell types.\n",
    "\n",
    "On day 3 of the workshop, you will learn about methods to map the morphology and synaptic connectivity of individual neurons, some of which are the same types of neurons recorded in this dataset. \n",
    "\n",
    "What kinds of questions can you address with 2-photon imaging alone? What kinds of questions could you address if you had both physiology and morphology? How can these different types of datasets inform each other?\n",
    " \n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27e1f2a4",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Multi-plane imaging across sensory & behavioral contexts</h4>\n",
    "\n",
    "<p>\n",
    "In in <b>Visual Behavior Ophys</b> dataset, a given population of neurons (i.e. a specific imaging plane) was measured across multiple sessions, and multiple imaging planes were recorded in each individual session. \n",
    "\n",
    "This experimental design allows analysis of changes in neural activity across days, under different <b>sensory and behavioral contexts</b>, and comparison of activity across different visual areas or cortical depths within a given session.\n",
    "\n",
    "In some ophys sessions, mice perform the task with the image set they saw during training, which is highly <b>familiar</b>. \n",
    "In other sessions, mice perform the task with <b>novel</b> images they have never seen before. \n",
    "\n",
    "In addition to <b>active behavior</b> sessions where mice perform the task to earn rewards, there are also <b>passive viewing</b> sessions where the mice observe the stimulus in open loop, with no rewards delivered. In these passive sessions, mice are satiated and were given their daily water ration prior to the imaging session. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b053061",
   "metadata": {},
   "source": [
    "![data_structure.png](../../resources/data_structure.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fc5d3db",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "These different features of the experiment means we need to keep track of a few things when doing analysis: \n",
    "<p>\n",
    "\n",
    "* Which <b>session type</b> we are looking at (what image set was used? was it an active or passive session?)\n",
    "  \n",
    "* Which <b>brain area and cortical depth</b> the cells are from (i.e. which imaging plane it is within a session?)\n",
    "  \n",
    "* Which <b>genetically defined cell population</b> was imaged (i.e. what is the genotype of the mouse?)                                                                      \n",
    "\n",
    "This information is provided as metadata by the <b>AllenSDK</b> toolkit, which you will learn how to use below.                                                                                                                                        \n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f968f8a",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "In the nomenclature of the <b>AllenSDK</b> we refer to each imaging plane within each session as an `ophys_experiment`.\n",
    "\n",
    "The population of neurons in each imaging plane was tracked across multiple `ophys_sessions`, recorded on different days.\n",
    "\n",
    "The collection of recording sessions belonging to a given imaging plane is called an `ophys_container`.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d4fbc6d",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "For today's workshop, we will analyze an experiment from a transgenic mouse expressing GCaMP6f in \n",
    "\n",
    "<b>Sst neurons</b> in the \n",
    "\n",
    "<b>primary visual cortex</b> during an \n",
    "\n",
    "<b>active behavior</b> session with \n",
    "\n",
    "<b>familiar images</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438caaf",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3> Data access </h3>\n",
    "\n",
    "(3) How can we access the data we are interested in?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276d05bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import these modules to get started\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn makes pretty plots & sets font sizes nicely\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', font_scale=1.5, rc={'lines.markeredgewidth': 2})\n",
    "\n",
    "# magic functions for jupyter notebook plotting\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91f3a03d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4>Using the AllenSDK toolkit</h4>\n",
    "\n",
    "<p>\n",
    "To identify experiments of interest based on the features of this dataset as described above, such as what cell populations were imaged, what types of sessions there were, etc., we need to access the metadata tables in the `VisualBehaviorOphysProjectCache` using the `AllenSDK` toolkit.\n",
    "\n",
    "The `VisualBehaviorOphysProjectCache` class is responsible for downloading any requested data or metadata as needed and storing it in well known locations.  For this workshop, all of the data has been preloaded into data assets on CodeOcean - These data are big, and this will save us a lot of bandwidth and time.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97faa4e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T19:17:31.537564Z",
     "iopub.status.busy": "2023-07-31T19:17:31.537208Z",
     "iopub.status.idle": "2023-07-31T19:17:31.561273Z",
     "shell.execute_reply": "2023-07-31T19:17:31.560402Z"
    },
    "papermill": {
     "duration": 0.044697,
     "end_time": "2023-07-31T19:17:31.563323",
     "exception": false,
     "start_time": "2023-07-31T19:17:31.518626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that you are currently using the newest version of SDK (2.16.2)\n",
    "import allensdk\n",
    "allensdk.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78ca57d2",
   "metadata": {
    "papermill": {
     "duration": 0.017273,
     "end_time": "2023-07-31T19:17:37.435519",
     "exception": false,
     "start_time": "2023-07-31T19:17:37.418246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The code below shows you how to use the `VisualBehaviorOphysProjectCache` class to load metadata tables & explore the features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ece77c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T19:17:37.471588Z",
     "iopub.status.busy": "2023-07-31T19:17:37.470961Z",
     "iopub.status.idle": "2023-07-31T19:17:37.502300Z",
     "shell.execute_reply": "2023-07-31T19:17:37.501229Z"
    },
    "papermill": {
     "duration": 0.052759,
     "end_time": "2023-07-31T19:17:37.505024",
     "exception": false,
     "start_time": "2023-07-31T19:17:37.452265",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This is the directory where files will be saved\n",
    "# If using Code Ocean, this should link to the data directory, where the files will already be available\n",
    "# output_dir = r'/scratch/'\n",
    "output_dir = r'/Users/marinag/Documents/Data/visual_behavior_ophys_cache_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65bcdc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinag/anaconda3/envs/swdb_2024/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import behavior projet cache class from SDK to be able to load the data\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache import VisualBehaviorOphysProjectCache\n",
    "\n",
    "cache = VisualBehaviorOphysProjectCache.from_s3_cache(cache_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cfe974",
   "metadata": {},
   "source": [
    "The cache contains methods that allow you to explore the types of recording sessions that exist in the dataset, and to load the data for individual experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa216e43",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "<h3> Metadata tables </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac07b631",
   "metadata": {},
   "source": [
    "#### Load all cache tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf58cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T19:17:37.612378Z",
     "iopub.status.busy": "2023-07-31T19:17:37.612018Z",
     "iopub.status.idle": "2023-07-31T19:17:40.020870Z",
     "shell.execute_reply": "2023-07-31T19:17:40.019872Z"
    },
    "papermill": {
     "duration": 2.430735,
     "end_time": "2023-07-31T19:17:40.023206",
     "exception": false,
     "start_time": "2023-07-31T19:17:37.592471",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There are 4 metadata tables associated with the Visual Behavior Ophys dataset\n",
    "behavior_session_table = cache.get_behavior_session_table()  \n",
    "ophys_session_table = cache.get_ophys_session_table()   \n",
    "ophys_experiment_table = cache.get_ophys_experiment_table()    \n",
    "ophys_cells_table = cache.get_ophys_cells_table()                         \n",
    "\n",
    "\n",
    "#print number of items in each table \n",
    "print('Number of behavior sessions = {}'.format(len(behavior_session_table)))\n",
    "print('Number of ophys sessions = {}'.format(len(ophys_session_table)))\n",
    "print('Number of ophys experiments = {}'.format(len(ophys_experiment_table)))\n",
    "print('Number of unique cells = {}'.format(len(ophys_cells_table.cell_specimen_id.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e096e29",
   "metadata": {},
   "source": [
    "What is the difference between the `ophys_session_table` and the `ophys_experiment_table`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d292fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ophys_experiment_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7954388",
   "metadata": {},
   "outputs": [],
   "source": [
    "ophys_session_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29600935",
   "metadata": {},
   "source": [
    "The `ophys_experiment_table` contains one row for each imaging plane recorded in each session for all mice in the dataset. \n",
    "\n",
    "The `ophys_session_table` contains one row for each ophys session, which can contain one or more imaging planes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8fb027c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<p>\n",
    "The metadata tables include a number of key details for understanding the dataset, such as - where the recordings were made, what type of cells were labeled, and what stimulus was shown in a given session. \n",
    "\n",
    "The *DataBook* describes all the columns of the metadata tables. We will explore a few of the most important ones here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78d6f3",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "First, let's narrow down our search and specifically look at the sessions for the <b>VisualBehaviorMultiscope</b> cohort. \n",
    "\n",
    "Filter the `ophys_session_table` to limit to the <b>VisualBehaviorMultiscope</b> `project_code` and assign the results to a new variable called `multiscope_sessions`.\n",
    "\n",
    "How many mice are in this cohort? What mouse genotypes are available? (Hint: get the unique values of the `full_genotype` column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to a specific cohort / project code and check how many mice there are\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what genotypes are available for this cohort\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1a6ba13",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "<h4>What is a genotype?</h4>\n",
    "    \n",
    "Typically, several transgenic lines of mice are bred together to create cell type specific expression of a gene of interest by combining a `driver line` (also called a `cre_line`) expressing Cre recombinase under the control of a specific gene of interest, and a `reporter line` that expresses some protein (such as GCaMP) under the control of Cre recombinase. This allows scientists to mix and match a variety of drivers & reporters to do different types of experiments. \n",
    "\n",
    "The `full_genotype`: describes the strategy that was used to label a given cell population with GCaMP.\n",
    "\n",
    "The `cre_line`: is the first element of the `full_genotype` and determines which cell population is being targeted. In this dataset, it can be Slc17a7 for excitatory neurons, or Sst or Vip for different types of inhibitory neurons.\n",
    "\n",
    "The `reporter_line`: is the final element of the `full_genotype` and determines what kind of reporter gene is expressed. In this dataset GCaMP6f is used.\n",
    "\n",
    "You can learn more about transgenic mice and associated techniques in the <b>*DataBook*</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffb40cb6",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Let's look at an experiment where Sst inhibitory neurons were recorded. \n",
    "\n",
    "What are the unique values of the `mouse_id` column for mice with `cre_line` = <b>Sst-IRES-Cre</b> in the `multiscope_sessions` table we just made?\n",
    "\n",
    "Pick the mouse with the largest value of `mouse_id` and assign it to a new variable called `special_mouse_id`.\n",
    "\n",
    "What are the available values of the `session_type` column for this mouse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8997cea",
   "metadata": {},
   "source": [
    "Note the data type of the `mouse_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by cre_line to get just the Sst mice, then print out the unique values of mouse_id, sorted in ascending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7259bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ID for the mouse with the largest number of mouse_id\n",
    "# This is our special mouse\n",
    "special_mouse_id = '546605'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all session types for our special mouse\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e022c4c9",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h4>What session type to choose?</h4>\n",
    "\n",
    "The `session_type` column is a short hand description that conveys a several pieces of information about what the mouse experienced during a given session. Some of these pieces of information also have their own unique columns that you can search by. The `session_type` includes: \n",
    "<p>\n",
    "\n",
    "* Whether the session was during <b>TRAINING</b> or <b>OPHYS</b> (the first element of the `session_type`)\n",
    "* Which `image_set` was shown during that session (the second element of the `session_type`)\n",
    "* The `behavior_type`, whether the session was <b>active behavior</b> or <b>passive viewing</b> (if the session type doesnt say `passive` at the end, that means it was an active behavior session)\n",
    "\n",
    "<p>\n",
    "Other columns that provide valuable information about what happened during a session include: \n",
    "<p>\n",
    "\n",
    "`experience_level`: whether the session used <b>Familiar</b> or <b>Novel</b> images, and whether it was the first novel day (`Novel 1`) or a subsequent novel day `Novel >1`\n",
    "\n",
    "`prior_exposures_to_image_set`: how many prior sessions the mouse has experienced with the image set being shown during the current session (should always be zero for `Novel 1` sessions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c370f865",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Get the `ophys_session_id` for `session_type` = `OPHYS_1_images_A` for our special mouse. Save it to a variable called `familiar_session_id`.\n",
    "\n",
    "What are the values of `experience_level` and `prior_exposures_to_image_set` for this session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92947cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the session metadata for special mouse with the session type listed above\n",
    "sessions = ophys_session_table[(ophys_session_table.mouse_id==special_mouse_id) & \n",
    "                                (ophys_session_table.session_type=='OPHYS_1_images_A')]\n",
    "sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the session ID to a variable called familiar_session_id\n",
    "familiar_session_id = sessions.index.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the experience_level and prior exposures values for this session?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d65ca01",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "<h4>Where was imaging performed for this session?</h4>\n",
    "\n",
    "Relevant metadata columns include: \n",
    "\n",
    "`imaging_depth`: Because Ca2+ imaging is an optical technique, recordings must be targeted to a specific focal depth of the microscope, corresponding to how deep in the tissue the images were collected. \n",
    "The values in the `imaging_depth` column indicate the distance from the cortical surface for each imaging plane that was recorded. \n",
    "\n",
    "`targeted_structure`: This is the brain area where the recording was made. \n",
    "In Allen Brain Observatory Ophys experiments, specific visual areas are targeted using Intrinsic Signal Imaging (ISI) to identify the boundaries of each visual area based on their reinotopic maps. You can learn more about this method in the <b>DataBook</b>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7470d541",
   "metadata": {},
   "source": [
    "As we saw previously, the `ophys_experiment_table` contains metadata for each individual image plane that was recorded in each session. Accordingly, information about which areas and depths were recorded can be found in the `ophys_experiment_table`, but not in the `ophys_session_table`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "338af4d7",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use the `ophys_experiment_table` to find all the imaging planes recorded in the `familiar_session_id` from our special mouse.\n",
    "\n",
    "What `targeted_structures` were imaged? What are the available values of `imaging_depth`? What `equipment_name` was used to record this session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all recordings for session type OPHYS_1_images_A for our special mouse using the ophys_session_id we saved above\n",
    "experiments = ophys_experiment_table[ophys_experiment_table.ophys_session_id==familiar_session_id]\n",
    "experiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af432624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the targeted structures and the imaging depths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look at both of these pieces of information at once by limiting the table just to those columns (plus a few others that might be interesting)\n",
    "experiments[['targeted_structure', 'imaging_depth', 'session_type', 'experience_level', 'equipment_name']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9d8a970",
   "metadata": {},
   "source": [
    "In this session, recordings were made in VISp and VISl, at multiple cortical depths. However we only see 4 imaging planes here (each represented by a unique `ophys_experiment_id`) - shouldnt we expect 8 imaging planes per session for multi-plane imaging experiments? \n",
    "\n",
    "While it is true that 8 imaging planes are recorded in each multi-plane imaging session (acquired using the `MESO.1` or `MESO.2` microscopes), there are strict quality control (QC) criteria applied to each imaging plane. \n",
    "\n",
    "Some of the 8 planes can fail QC while others pass. Examples of QC criteria include: how much brain motion there was for a given plane or whether the signal to noise was too low to detect cells."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e66aa662",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Get the `ophys_experiment_id` for the recording in `VISp` at `225`um depth, and save it to a variable called `ophys_experiment_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_structure = 'VISp'\n",
    "imaging_depth = 225\n",
    "# Get the ophys_experiment_id for this area and depth for our specific session\n",
    "ophys_experiment_id = experiments[(experiments.targeted_structure==targeted_structure) & \n",
    "                                    (experiments.imaging_depth==imaging_depth)].index.values[0]\n",
    "print(ophys_experiment_id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "834ca007",
   "metadata": {
    "papermill": {
     "duration": 0.021281,
     "end_time": "2023-07-31T19:17:40.488522",
     "exception": false,
     "start_time": "2023-07-31T19:17:40.467241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "<h3> Physiology data </h3>\n",
    "\n",
    "To load the data for a single imaging plane recorded in a given session, we can use the `get_behavior_ophys_experiment` method of the `VisualBehaviorOphysProjectCache` class that we instantiated previously as `cache`. \n",
    "\n",
    "This method returns a python object that contains all data and metadata for a given recording as attributes, along with some useful functions. We typically name this python object simply `dataset`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eb8bb10",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use the `ophys_experiment_id` we saved above as the input to the `get_behavior_ophys_experiment` method of the cache. \n",
    "\n",
    "Save the output to a variable called `dataset`. This is a python object that contains all the data for this imaging plane. \n",
    "\n",
    "Examine the `metadata` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3483d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T19:17:40.579330Z",
     "iopub.status.busy": "2023-07-31T19:17:40.578765Z",
     "iopub.status.idle": "2023-07-31T19:17:56.545599Z",
     "shell.execute_reply": "2023-07-31T19:17:56.544436Z"
    },
    "papermill": {
     "duration": 15.994575,
     "end_time": "2023-07-31T19:17:56.548301",
     "exception": false,
     "start_time": "2023-07-31T19:17:40.553726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset for the ophys_experiment_id we selected \n",
    "dataset = cache.get_behavior_ophys_experiment(ophys_experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f06e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T19:17:56.647173Z",
     "iopub.status.busy": "2023-07-31T19:17:56.646806Z",
     "iopub.status.idle": "2023-07-31T19:17:56.678912Z",
     "shell.execute_reply": "2023-07-31T19:17:56.677952Z"
    },
    "papermill": {
     "duration": 0.059981,
     "end_time": "2023-07-31T19:17:56.681515",
     "exception": false,
     "start_time": "2023-07-31T19:17:56.621534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at the metadata attribute\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0117f7f",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h4>What physiology data is provided?</h4>\n",
    "<p>\n",
    "\n",
    "`dff_traces`: dataframe containing normalized fluorescence traces for each cell. dF/F or dFF stands for 'delta fluorescence over baseline fluorescence', i.e. the change in fluorescence relative to each cell's baseline signal. \n",
    "\n",
    "`events`: dataframe containing calcium events detected from fluorescence signals. events are detected based on the rapid rise in calcium, typically associated with bursts of spikes. Events have a time and a magnitude, roughly equivalent to the spike rate of a neuron.\n",
    "\n",
    "`ophys_timestamps`: time, in seconds, of each imaging frame of the 2-photon movie. The indices of `dff_traces` and `events` correspond to the times in the `ophys_timestamps` array. Note that the frame rate of the recordings can vary, with 30Hz being typical for single-plane imaging sessions, and 11Hz typical for multi-plane imaging.\n",
    "\n",
    "`max_projection`: array of maximum intensity projection image of the 2-photon movie. Allows visualization of pixels with large changes in fluorescence, corresponding to active neurons.  \n",
    "\n",
    "`average_projection`: array of average intensity projection image of the 2-photon movie. Allows visualization of average fluorescence across the 2-photon field of view. \n",
    "\n",
    "`roi_masks`: dataframe containing regions of interest corresponding to neuron cell bodies, segmented from the 2-photon movies. Each cell trace comes from one of the roi_masks.\n",
    "\n",
    "`segmentation_mask_image`: array containing all segmented ROIs.\n",
    "\n",
    "`cell_specimen_table`: dataframe containing cell ROI information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cbfccf8",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the `max_projection` and `segmentation_mask_image` this imaging plane. How many ROIs are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b786ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the max projection image with plt.imshow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f56aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T19:17:56.831045Z",
     "iopub.status.busy": "2023-07-31T19:17:56.829909Z",
     "iopub.status.idle": "2023-07-31T19:17:57.074763Z",
     "shell.execute_reply": "2023-07-31T19:17:57.073890Z"
    },
    "papermill": {
     "duration": 0.272642,
     "end_time": "2023-07-31T19:17:57.076963",
     "exception": false,
     "start_time": "2023-07-31T19:17:56.804321",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the segmentation_mask_image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa63702c",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Examine the `dff_traces` and `events` attributes. How are they formatted?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff142e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T19:17:58.043650Z",
     "iopub.status.busy": "2023-07-31T19:17:58.042853Z",
     "iopub.status.idle": "2023-07-31T19:17:58.095082Z",
     "shell.execute_reply": "2023-07-31T19:17:58.094220Z"
    },
    "papermill": {
     "duration": 0.082713,
     "end_time": "2023-07-31T19:17:58.097041",
     "exception": false,
     "start_time": "2023-07-31T19:17:58.014328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at dff_traces attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7209321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T19:17:58.216858Z",
     "iopub.status.busy": "2023-07-31T19:17:58.216090Z",
     "iopub.status.idle": "2023-07-31T19:17:58.253005Z",
     "shell.execute_reply": "2023-07-31T19:17:58.252017Z"
    },
    "papermill": {
     "duration": 0.068181,
     "end_time": "2023-07-31T19:17:58.255456",
     "exception": false,
     "start_time": "2023-07-31T19:17:58.187275",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at events attribute\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dd70f60",
   "metadata": {
    "papermill": {
     "duration": 0.028505,
     "end_time": "2023-07-31T19:17:58.865738",
     "exception": false,
     "start_time": "2023-07-31T19:17:58.837233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "The ```events``` table is similar to ```dff_traces``` but the output provides traces of deconvolved events. Events are computed on spatially unmixed dff traces for each cell as described in [Giovannucci et al. 2019](https://pubmed.ncbi.nlm.nih.gov/30652683/). \n",
    "\n",
    "The magnitude of events approximates the firing rate of neurons with the resolusion of about 200 ms. The biggest advantage of using events over dff traces is they exclude prolonged Ca transients that may conteminate neural responses to subsequent stimuli. You can also use ```filtered_events``` which are events convolved with a filter created using ```stats.halfnorm``` method. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d929a2a5",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Pick one `cell_specimen_id` and plot dF/F and events for that cell, using `ophys_timestamps` for the y-axis values to show the time in seconds. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2ae0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T19:17:59.130776Z",
     "iopub.status.busy": "2023-07-31T19:17:59.130214Z",
     "iopub.status.idle": "2023-07-31T19:17:59.162342Z",
     "shell.execute_reply": "2023-07-31T19:17:59.161310Z"
    },
    "papermill": {
     "duration": 0.06497,
     "end_time": "2023-07-31T19:17:59.165288",
     "exception": false,
     "start_time": "2023-07-31T19:17:59.100318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's try the second cell (index one of the dff_traces and events tables)\n",
    "\n",
    "# Get cell_specimen_ids from the cell_specimen_table. Can also get from either the dff_traces or events\n",
    "cell_specimen_ids = dataset.dff_traces.index.values # a list of all cell ids\n",
    "cell_specimen_id = cell_specimen_ids[1] # pick the second cell (index = 1 because python uses zero indexing)\n",
    "print('Cell specimen id = {}'.format(cell_specimen_id)) # print the cell ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08e303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T19:17:59.233323Z",
     "iopub.status.busy": "2023-07-31T19:17:59.232875Z",
     "iopub.status.idle": "2023-07-31T19:17:59.696230Z",
     "shell.execute_reply": "2023-07-31T19:17:59.695219Z"
    },
    "papermill": {
     "duration": 0.501785,
     "end_time": "2023-07-31T19:17:59.699584",
     "exception": false,
     "start_time": "2023-07-31T19:17:59.197799",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot dff and events traces overlaid on the same axis for the cell selected above\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3884662",
   "metadata": {
    "papermill": {
     "duration": 0.032889,
     "end_time": "2023-07-31T19:17:59.765091",
     "exception": false,
     "start_time": "2023-07-31T19:17:59.732202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see that as expected, events trace is much cleaner than dff and it generally follows big calcium transients really well.\n",
    "\n",
    "This cell is particularly active towards the end of the session - whats up with that? \n",
    "\n",
    "If you've checked out the <b>*DataBook*</b> description of the Visual Behavior Ophys dataset, you would have seen that each Visual Behavior Ophys experiment has 10 repeats of a 30 second movie clip at the end of each session. \n",
    "It would be interesting to see how reliable the cell's response is across repeats of the movie. \n",
    "\n",
    "You can also learn about what stimuli are presented and when using the `stimulus_presentations` table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92b7794d",
   "metadata": {
    "papermill": {
     "duration": 0.0349,
     "end_time": "2023-07-31T19:18:01.101548",
     "exception": false,
     "start_time": "2023-07-31T19:18:01.066648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "<h3> Stimulus presentations </h3>\n",
    "\n",
    "Each ophys session is broken up into several <b>stimulus blocks</b>.\n",
    "\n",
    "First, a 5 minute gray screen period occurs during which there are no visual stimuli. This is helpful to determine cells' baseline level of activity. There is another 5 minute gray screen period at the end of the session, followed by 10 repeats of a 30 second movie clip. \n",
    "\n",
    "The bulk of each ophys session is change detection task performance, which lasts for 60 minutes. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c95672d",
   "metadata": {},
   "source": [
    "![vbo_session_structure_2.png](../../resources/vbo_session_structure_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "213d3f99",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Examine the `stimulus_presentations` attribute of the dataset. What are the columns? \n",
    "\n",
    "What are the values of the `stimulus_block_name` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f6a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the stimulus_presentations table look like? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c55d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the stimulus blocks?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa6d379b",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Assign the table to a variable called `stimulus_presentations`, so that we dont have to retrieve it from the dataset object every time we want to use it.\n",
    "\n",
    "Select the `change_detection_behavior` block and look at the unique values of the `image_name` column for that block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the table to a variable\n",
    "stimulus_presentations = dataset.stimulus_presentations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706adf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to change detection behavior block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f307e",
   "metadata": {},
   "source": [
    "This table provides helpful information like image name, start, duration and stop of image presentation, and whether the image was omitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the image names?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47312b27",
   "metadata": {},
   "source": [
    "If you are curious what these images look like, you can check the <b>*DataBook*</b> to learn how to visualize them using the `stimulus_templates` attribute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "005be971",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "<h4> Image changes and image omissions </h4>\n",
    "\n",
    "You may have noticed that one of the values of `image_name` is \"omitted\". \n",
    "That is because some image presentations are randomly omitted during ophys sessions (but never during training).\n",
    "This allows neural signals associated with the absence of an expected stimulus to be analyzed. \n",
    "\n",
    "The `omitted` column of the `stimulus_presentations` table also provides a useful Boolean value to filter by omissions. \n",
    "\n",
    "Another useful column is the `is_change` column, which is another Boolean value. \n",
    "This can be used to identify the image changes, which are the <b>go</b> trials of this task. \n",
    "\n",
    "You can also look at <b>no-go</b> or <b>catch</b>  trials using the `is_sham_change` column. \n",
    "This column is True for all image presentations that could have been a change, according to the exponential distribution of change times. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb0d5dba",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "How many unique `stimulus_presentations` are there in this session?\n",
    "\n",
    "How many image changes were there? How many stimuli were omitted? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all stimulus presentations\n",
    "print(len(stimulus_presentations), 'stimulus presentations total')\n",
    "\n",
    "# Count the changes\n",
    "print(len(stimulus_presentations[stimulus_presentations.is_change==True]), 'stimulus presentations were changes')\n",
    "\n",
    "# Count the omissions\n",
    "print(len(stimulus_presentations[stimulus_presentations.omitted==True]), 'stimulus presentations were omitted')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6027545e",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "<h4> Timestamps </h4>\n",
    "\n",
    "Now that we know how to get the stimuli for this session, we want to ask how neurons respond to different types of stimuli. \n",
    "This means we need to know when a given stimulus happened relative to the neural recordings. \n",
    "\n",
    "All the data in each session was recorded on a common clock, however not all data streams were sampled at the same rate. \n",
    "Let's examine the timestamps to understand the differences\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9a1f41a",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Examine the `stimulus_timestamps` attribute. Compare it to the values of `ophys_timestamps`. Are they the same? \n",
    "\n",
    "Compute the frame rate of each set of timestamps by using `np.diff` to get the inter-frame interval. The frame rate is 1 divided by the average inter-frame interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9459b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stimulus timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ophys timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stimulus frame rate\n",
    "1/np.mean(np.diff(dataset.stimulus_timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c93e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ophys frame rate\n",
    "1/np.mean(np.diff(dataset.ophys_timestamps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457b5f8",
   "metadata": {},
   "source": [
    "Note that the stimulus frames and ophys frames are acquired at different frame rates, however they are recorded on the same time clock."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc973d66",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "<h3> Getting stimulus aligned responses </h3>\n",
    "\n",
    "As we saw above, the ophys data and stimulus presentations are not recorded at the same rate. \n",
    "If we want to compute stimulus aligned cell activity, we will need a way to associate ophys timestamps with the nearest stimulus timestamps. \n",
    "\n",
    "Fortunately, the `brain_observatory_utilities` package provides tools to make this easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9525d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brain_observatory_utilities.datasets.optical_physiology.data_formatting as data_formatting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49c52456",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "We can use the `get_stimulus_response_df` function from the `datasets.optical_physiology.data_formatting` module to get the stimulus locked activity for all cells in the dataset. \n",
    "\n",
    "Let's look at the documentation for this function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the documentation for the get_stimulus_response_df function\n",
    "data_formatting.get_stimulus_response_df?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4571e662",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "The `get_stimulus_response` function is smart and already knows what our data structures look like, \n",
    "so it can pull out the relevant information from the cell activity tables (`dff_traces` and `events`) \n",
    "and from the `stimulus_presentations` table.\n",
    "\n",
    "The `ophys_experiment` argument to the `get_stimulus_response` function is an instance of the dataset object that we have been working with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc8ea912",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use the `get_stimulus_response_df` function to get stimulus aligned dff traces for image changes, in a +/- 1s window around the change time.\n",
    "\n",
    "What are the columns of the table that is returned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stimulus aligned dff traces (using data_type input) for image changes (using event_type input)\n",
    "# For a +/-1 second window (using time_window input)\n",
    "# The default response_window_duration is 0.5, which means the average in a 0.5 second window after the stimulus onset will be computed\n",
    "# interpolating the traces to 30Hz ensures that the timestamps are nicely consistent across trials, but this isnt strictly necessary\n",
    "stimulus_response_df = data_formatting.get_stimulus_response_df(dataset, data_type='dff', event_type='changes',\n",
    "                                                            time_window=[-1, 1], response_window_duration=0.5,\n",
    "                                                            interpolate=True, output_sampling_rate=30)\n",
    "stimulus_response_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff692db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the columns of your new stimulus response dataframe? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "814ad492",
   "metadata": {},
   "source": [
    "Note that the stim_response_df contains the index of the `stimulus_presentations` table, the `stimulus_presentations_id`. \n",
    "\n",
    "This means that we can easily add stimulus information to the stimulus response dataframe by merging it with the `stimulus_presentations` table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e73ec13",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Add stimulus metadata by merging the `stimulus_response_df` and the `stimulus_table` using the `stimulus_presentations_id` column. \n",
    "\n",
    "Check the documentation for the `pandas.merge` function if you are unsure how to use it. https://pandas.pydata.org/docs/reference/api/pandas.merge.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the stimulus_presentations table with the stimulus_response_df\n",
    "stimulus_response_df = stimulus_response_df.merge(stimulus_presentations, on='stimulus_presentations_id')\n",
    "stimulus_response_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "359b6e8a",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "What columns exist in your new `stimulus_response_df`? \n",
    "\n",
    "What are the values of the `is_change` column (which we added via the merge function above)?\n",
    "\n",
    "What are the values of the `trace_timestamps` column? Are they all the same? What are they referenced to? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3497e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the columns available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the values of the is_change column? Why are these the only values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b257479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whats up with the timestamps?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3229e",
   "metadata": {},
   "source": [
    "Looks like the timestamps go from -1 to 1 second around the stimulus onset time, just like we asked for when we ran the `get_stimulus_response_df` function!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1789762",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the average image change evoked response for the `cell_specimen_id` we selected earlier, using the `trace` and `trace_timestamps` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the stimulus aligned trace for the cell_specimen_id we care about\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16b3dd25",
   "metadata": {},
   "source": [
    "Looks like this cell reduces its activity after an image change. I wonder if thats a property of all Sst neurons, or just this one... \n",
    "\n",
    "What about other cell types like Vip inhibitory neurons or Slc17a7 excitatory neurons? Are they change modulated?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86d7cb3f",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Hurray!! We can exact stimulus evoked responses! Now we can finally start asking the questions we outlined at the beginning!! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a5e7236",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<h2> Part 2 -  Tuning for stimulus & behavior during task performance </h2>\n",
    "\n",
    "(1) Are neurons in the mouse visual cortex selective for specific visual stimuli? How reliable are their responses?\n",
    "\n",
    "(2) Do stimulus responses differ depending on the mouse's behavioral choice during the task? \n",
    "\n",
    "(3) Do neurons in the mouse visual cortex modulate their activity as a function of running speed? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3990737",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "To understand sensory tuning, we want to know how individual cells respond to different stimuli. We will test this out first.\n",
    "\n",
    "To understand the impact of behavior on single cell coding, we could take a few different approaches. \n",
    "\n",
    "One aspect of <b>behavior</b>  is the animals behavioral <b>choice</b> during the task. \n",
    "We could ask whether stimulus tuning, or overall cell activity, is different for image changes when the mouse correctly responds to the change with a lick and got a reward(i.e. a <b>hit</b> trial), compared with changes where the mouse failed to respond (i.e. <b>miss</b> trials)\n",
    "\n",
    "Another aspect of <b>behavior</b> is the animals overall behavioral state, such as whether they are <b>running</b> or <b>stationary</b>. \n",
    "Here we could ask whether a given cell's activity level is modulated by the overall speed of the mouse (i.e. are cells \"tuned\" for running speed?). Alternatively, we could look at how cell activity varies with <b>pupil diameter</b>, which is a measure of arousal and is correlated with various measures of brain state. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d73824f5",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h3> Stimulus tuning & response variability </h3>\n",
    "\n",
    "Let's start by asking whether individual cells respond differently to the 8 different images shown in each Visual Behavior Ophys session\n",
    "\n",
    "Then we will evaluate how consistent that response is, and whether it is valid to claim that the cell \"encodes\" a given image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2b699",
   "metadata": {},
   "source": [
    "Let's revisit the `stimulus_response_df` for the `cell_specimen_id` we are interested in.\n",
    "\n",
    "The `mean_response` column contains the average value of the dF/F signal (which is what we provied as the `data_type` to the `get_stimulus_response_df` function - we could replace that with `events` to use deconvolved events instead) in a pre-defined window of time following the stimulus onset (determined by the value of `response_window_duration` provided to the `get_stimulus_response_df` function above). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7aa6a85",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Get the data from `stimulus_response_df` just for our `cell_specimen_id` of interest and assign it to its own variable.\n",
    "\n",
    "Get the average value of the `mean_response` column for each unique `image_name` in the `stimulus_response_df` for our cell and plot it. \n",
    "\n",
    "The y-axis should be the value of the `mean_response` and the x-axis should be the `image_name`. \n",
    "\n",
    "Bonus points for using `pandas.groupby` for this: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcfff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for our cell, which we should have saved to a variable called cell_specimen_id above\n",
    "cell_df = stimulus_response_df[stimulus_response_df.cell_specimen_id==cell_specimen_id]\n",
    "cell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average of mean response column for each image\n",
    "# You could do this using a for loop, but using pandas groupby is better\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ae689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean response for each image \n",
    "# Make sure to label your axes!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd1f493a",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now plot the `mean_response` for each individual presentation of each image, along with the average response across all repetitions of each image as we did above. \n",
    "\n",
    "How variable is the cell activity across repeated presentations of a given image?\n",
    "\n",
    "Bonus points for using `seaborn.scatterplot` function with the `stimulus_response_df` dataframe for this cell as the input: https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bf00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean response for each image\n",
    "\n",
    "# Get cell data and compute tuning curve using groupby\n",
    "cell_df = stimulus_response_df[stimulus_response_df.cell_specimen_id==cell_specimen_id]\n",
    "tuning_curve = cell_df.groupby(['image_name']).mean()[['mean_response']]\n",
    "\n",
    "# Get sorted image names for x-axis\n",
    "image_names = np.sort(tuning_curve.index.values)\n",
    "\n",
    "# Make the plot\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax = sns.scatterplot(data=cell_df, x='image_name', y='mean_response', ax=ax)\n",
    "ax = sns.pointplot(data=cell_df, x='image_name', y='mean_response', order=image_names, color='r', linestyle='--', ax=ax)\n",
    "ax.set_title('cell_specimen_id: '+str(cell_specimen_id)+'\\nImage selectivity')\n",
    "ax.set_xticklabels(image_names, rotation=90)\n",
    "ax.set_ylabel('dF/F')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58bf30d5",
   "metadata": {},
   "source": [
    "What could account for the trial to trial variability here? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f13a129",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "What do other cells' tuning curves look like? \n",
    "\n",
    "Get the average image response for all cells in this experiment and plot it as a heatmap. \n",
    "\n",
    "Super mega bonus points if you use `pandas.groupby`, `pandas.pivot_table`, AND `seaborn.heatmap` for this\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html\n",
    "\n",
    "https://seaborn.pydata.org/generated/seaborn.heatmap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average response across all presentations of each image for each cell in this session using pandas groupby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pivot_table to convert it into a matrix of cells by images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3189da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot that matrix using seaborn's heatmap function\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "546432f2",
   "metadata": {},
   "source": [
    "Does this pattern reflect the true selectivity of these cells, or is it just random chance? \n",
    "\n",
    "One way to test thais is to ask whether image tuning differs depending on which trials you select\n",
    "\n",
    "Let's try splitting the data, then plotting the same heatmap for the splits. Will it look the same?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a062217",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Splitting the data </h4>\n",
    "\n",
    "Let's try splitting the data and checking if the tuning curves look the same when computed separately for the first and second halves of the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd935b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of stimulus presentations and get the halfway point in the session\n",
    "stimulus_presentation_ids = stimulus_response_df.stimulus_presentations_id.unique()\n",
    "print('there are', len(stimulus_presentation_ids), 'stimulus presentations in this dataframe')\n",
    "print('the halfway point is ~ trial', int(len(stimulus_presentation_ids)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into first and second half using this threshold on stimulus_presentations_id\n",
    "first_half = stimulus_response_df[stimulus_response_df.stimulus_presentations_id.isin(np.arange(0, 156))]\n",
    "second_half = stimulus_response_df[stimulus_response_df.stimulus_presentations_id.isin(np.arange(156, len(stimulus_presentation_ids)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd8b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tuning curves for the first half, using groupby and pivot_table as we did above\n",
    "tuning_curves_first_half = first_half.groupby(['cell_specimen_id', 'image_name']).mean()[['mean_response']]\n",
    "tuning_curves_first_half = tuning_curves_first_half.pivot_table(values='mean_response', index='cell_specimen_id', columns='image_name')\n",
    "\n",
    "# Compute tuning curves for the second half\n",
    "tuning_curves_second_half = second_half.groupby(['cell_specimen_id', 'image_name']).mean()[['mean_response']]\n",
    "tuning_curves_second_half = tuning_curves_second_half.pivot_table(values='mean_response', index='cell_specimen_id', columns='image_name')\n",
    "\n",
    "# Plot the heatmap for the tuning curves computed on each split of the data\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,4), sharey=True)\n",
    "ax[0] = sns.heatmap(data=tuning_curves_first_half, vmax=0.5, cbar=True, ax=ax[0])\n",
    "ax[0].set_title('first half of trials')\n",
    "ax[1] = sns.heatmap(data=tuning_curves_second_half, vmax=0.5, cbar=True, ax=ax[1])\n",
    "ax[1].set_title('second half of trials')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59b8c775",
   "metadata": {},
   "source": [
    "These look pretty different!! What could cause these differences? \n",
    "\n",
    "One factor could be task engagement. Mice often perform the task better in the first half than the second half, because they are more motivated. \n",
    "When they are engaged, most of the trials are hits. When the mice disengage, they have a lot more misses.\n",
    "\n",
    "We can split the data by hit and miss trials to see whether the mouse's behavioral choice influences tuning. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18b117af",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h3> Does behavior choice affect stimulus response? </h3>\n",
    "\n",
    "Let's try splitting the data based on whether each image change resulted in a <b>hit</b> or a <b>miss</b> and see if the mouse's behavioral choice influences the response of our cell of interest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b94726",
   "metadata": {},
   "source": [
    "![Trial_diagram.png](../../resources/Trial_diagram.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7988445",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Above we looked at responses to individual images that were shown during the session. \n",
    "Now we want to look at image changes that were either a <b>hit</b>, where the mouse correctly licked following the change, or a <b>miss</b>, where the mouse failed to lick after the change. \n",
    "\n",
    "First we need to figure out whether the mouse correctly licked following each image change or not\n",
    "\n",
    "We could do this by determining whether there was a lick or a reward for each trial. Let's look at the `licks` and `rewards` attributes of the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de17d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the licks attribute of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the rewards attribute\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3b2440f",
   "metadata": {},
   "source": [
    "To figure out which image changes had a correct lick or not (and thus correspond ot a hit or a miss), we would need to compare the onset times of each image change in the `stimulus_presentations` table to the lick times in the `licks` table (or the reward times in the `rewards` table) to see if there was a lick (or a reward) within 750ms of the stimulus onset. \n",
    "\n",
    "This is technically straightforward but can be tedious, so to save some time so that you can focus on asking interesting questions rather than data munging, we will provide you with some tools to annotate the `stimulus_presentations` table with things like lick and reward times for each image presentation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb565354",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h4> Annotating stimulus presentations with behavior information </h4>\n",
    "<p>\n",
    "\n",
    "The `brain_observatory_utilities` package provides a useful tool to annotate the `stimulus_presentations` table with information about what happened during each stimulus, including timing of `licks`, `rewards`, and whether the trial was a <b>hit</b> or a <b>miss</b> trial. \n",
    "\n",
    "It will also add the average `running_speed` and `pupil_width` for each stimulus presentation. These can be used to filter data, or plot directly against cell activity to ask about he relationship between running and neural activity. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f184b81",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "The `get_annotated_stimulus_presentations` function can be found in the `datasets.behavior.data_formatting` module of `brain_observatory_utilities`. It takes in the <b>dataset</b> object, which contains everything it needs to know about stimulus presentations, licks, rewards, running, etc., and returns an annotated version of the `stimulus_presentations` table.\n",
    "\n",
    "Let's check out the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brain_observatory_utilities.datasets.behavior.data_formatting as behavior_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_utils.get_annotated_stimulus_presentations?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa80705a",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use the `get_annotated_stimulus_presentations` function to get the annotated stimulus presentations table and assign it to a variable called `annotated_stim_table`. Inspect the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d07adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide dataset object to the get_annotated_stimulus_presentations function and assign the results to a new variable\n",
    "annotated_stim_table = behavior_utils.get_annotated_stimulus_presentations(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b877bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all the useful new columns!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5600737",
   "metadata": {},
   "source": [
    "To be able to sort cell activity based on whether each image change in the stimulus table was a hit or a miss, we will want to merge the `stimulus_response_df` and with this new annotated table. Let's recompute the stimulus response dataframe and merge it with the `annotated_stim_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e242ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stimulus response dataframe just for image changes\n",
    "stimulus_response_df = data_formatting.get_stimulus_response_df(dataset, data_type='dff', event_type='changes',\n",
    "                                                            time_window=[-1, 1], response_window_duration=0.5,\n",
    "                                                            interpolate=True, output_sampling_rate=None)\n",
    "\n",
    "# Merge it with the annotated stim table so you can filter cell responses based on behavior choice                                                       \n",
    "stimulus_response_df = stimulus_response_df.merge(annotated_stim_table, on='stimulus_presentations_id')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac72e15c",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "Now we can plot tuning curves separately for hits and misses.\n",
    "\n",
    "Limit the `stimulus_response_df` to image changes, then split by hit & miss trials. \n",
    "\n",
    "Plot our cell's image tuning curve (and the variability around the mean) using seaborn's pointplot, showing hits and misses using different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22110260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get responses just for our cell, then split into hit and miss trials\n",
    "cell_df = stimulus_response_df[stimulus_response_df.cell_specimen_id==cell_specimen_id]\n",
    "hits = cell_df[(cell_df.is_change)&(cell_df.hit)]\n",
    "misses = cell_df[(cell_df.is_change)&(cell_df.miss)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ece60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean response for each image split by hit and miss\n",
    "\n",
    "# Make sure that the x-axis is sorted by image name\n",
    "image_names = np.sort(cell_df.image_name.unique())\n",
    "\n",
    "# Plot hits and misses\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax = sns.pointplot(data=hits, x='image_name', y='mean_response', order=image_names, color='r', linestyle='--', ax=ax)\n",
    "ax = sns.pointplot(data=misses, x='image_name', y='mean_response', order=image_names, color='b', linestyle='--', ax=ax)\n",
    "ax.set_title('cell_specimen_id: '+str(cell_specimen_id)+'\\nImage selectivity')\n",
    "ax.set_xticklabels(image_names, rotation=90)\n",
    "ax.set_ylabel('dF/F')\n",
    "ax.legend(['hits', 'misses'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could plot the same thing using the 'hue' input to seaborn pointplot\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax = sns.pointplot(data=cell_df[cell_df.is_change], x='image_name', y='mean_response', hue='hit', order=image_names, linestyle='--', ax=ax)\n",
    "ax.set_title('cell_specimen_id: '+str(cell_specimen_id)+'\\nImage selectivity')\n",
    "ax.set_xticklabels(image_names, rotation=90)\n",
    "ax.set_ylabel('dF/F')\n",
    "ax.legend(title='Hit')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dafdd7b1",
   "metadata": {},
   "source": [
    "What if it isnt actually the mouse's choice thats influencing the differences in activity seen here? \n",
    "\n",
    "What if the mouse runs more or less in the first and second half of the experiment, or if the pupil diameter is correlated with whether or not the mouse receives a reward or not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89398281",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h3> Are cells tuned for running speed or pupil diameter? </h3>\n",
    "\n",
    "Another factor that could influence neural variability and contribute to neural encoding is locomotor behavior, or the behavioral state of the animal. Many studies have shown that animal movement and overall arousal state can influence the gain of sensory tuning. Running and other movements are also directly encoded by some neurons in the visual cortex, independent of stimulus identity. \n",
    "\n",
    "The dataset object contains info about the mouse's `running_speed`, in addition to information about pupil diameter and gaze location in the `eye_tracking` attribute. Running speed and pupil diameter are typically correlated, and both can be used as measures of overall arousal and behavioral state. \n",
    "\n",
    "Let's plot the activity of our Sst cells as a function of the mouse's running speed or pupil diameter to see if these neurons encode behavioral variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9aa7608",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Get the `running_speed` attribute of the dataset object and plot the running speed. \n",
    "\n",
    "Remember that running speed is sampled at the stimulus display frequency, so you can use `stimulus_timestamps` to plot time on the x-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the running speed, with stimulus_timestamps on x-axis\n",
    "fig, ax = plt.subplots(1,1, figsize = (20,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef70650",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Examine the `eye_tracking` attribute of the dataset object. What are the columns? \n",
    "\n",
    "Plot `pupil_area` over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whats in the eye_tracking table?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pupil area over time\n",
    "fig, ax = plt.subplots(1,1, figsize = (20,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de4dae",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "A note about filtering and signal processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "addc8e7c",
   "metadata": {},
   "source": [
    "Note that there are some very large spikes in the pupil area in some parts of the session. These are probably artifacts of the pupil detection algorithm, and could be filtered out using `scipy.signal.medfilt`: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.medfilt.html. This function works by setting each point to be the median of its immediate neighborhood of `kernel_size` points, so is a good tool for data with obvious outliers. \n",
    "\n",
    "Note that because kernel size is measured in points rather than timebins, this function makes the most sense to use when sampling rates are fairly consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (20,5))\n",
    "fit_pupil_area = medfilt(dataset.eye_tracking.pupil_area, kernel_size=21)   \n",
    "ax.plot(dataset.eye_tracking.timestamps, dataset.eye_tracking.pupil_area, color='magenta')\n",
    "ax.plot(dataset.eye_tracking.timestamps, fit_pupil_area, color='gray')\n",
    "\n",
    "ax.set_xlabel('Time in session (seconds)')\n",
    "ax.set_ylabel('Pupil area (pixels^2)')\n",
    "ax.set_title('Ophys experiment {}'.format(ophys_experiment_id), fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd00b2",
   "metadata": {},
   "source": [
    "Now that we have a handle on this behavior data, lets try plotting running speed and tuning together. Do they look like they might be related to one another?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7135459",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the `running_speed` and `pupil_area` on the same axies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56632de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (20,5))\n",
    "fit_pupil_area = medfilt(dataset.eye_tracking.pupil_area, kernel_size=21)   \n",
    "ax.plot(dataset.stimulus_timestamps, dataset.running_speed['speed'], color='orange')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(dataset.eye_tracking.timestamps, fit_pupil_area, color='magenta')\n",
    "\n",
    "ax.set_xlabel('Time in session (seconds)')\n",
    "ax.set_ylabel('Pupil area (pixels^2)')\n",
    "ax.set_title('Ophys experiment {}'.format(ophys_experiment_id), fontsize = 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aaf3ead6",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Tuning for continuous variables </h4>\n",
    "\n",
    "One challenge in working with these data is that running, eye tacking, and neural activity are all sampled on separate data streams with different timestamps. This means that even though these data were all collected at the same time, there isn't necessarily a one-to-one matchup between timestamps in one data stream or other.\n",
    "\n",
    "The most common solution to this solution to this problem is data resampling. Typically timestamp bins are defined, and data are resampled into a common time stream. What size bin should you use? This depends on the timescale that is relevant to the analysis at hand.\n",
    "\n",
    "For today, we will be using stimulus-presentation bins to look at our data over a relatively large timescale. Specifically, we will use the `stimulus_response_df` that we generated above, which contains the mean response for each stimulus presentation, along with the `annotated_stimulus_presentations` table that we merged into it, which contains the mean running speed for each stimulus presentation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb925e",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Get the stimulus response data for our special cell and plot `running_speed` and `mean_response` against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stimulus response dataframe just for a particular cell\n",
    "cell_df = stimulus_response_df[stimulus_response_df.cell_specimen_id==cell_specimen_id]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34bcbb66",
   "metadata": {},
   "source": [
    "Now that we have data binned by stimulus presentation, lets try plotting the relationship between running speed and the activity of our cell.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97caa359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot this cells mean response versus running speed for each stimulus presentation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14b31c26",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Repeat using the `mean_pupil_width` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean pupil width across trials against this cells mean response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26c256",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "\n",
    "Great! Our encodes both pupil diameter and running speed. \n",
    "\n",
    "You will recall, however, that these variables themselves are also correlated looked like they might have had a relationship to each other. Now that we have nicely binned data, try explicitly plotting the relationship between `mean_pupil_width` and `mean_running_speed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pupil width and running speed binned by stimulus presentations against each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff4852",
   "metadata": {},
   "source": [
    "So...which of the factors best explains the variability in trial-to-trial responses of our cell? \n",
    "\n",
    "In the next section, we will dive deeper into this problem. Specifically, we will use regression models to pull apart the variance explained by different concurrent features on their own, as well as their interactions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae0ef81d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<h2> Part 3 -  Quantifying single cell coding with regression models </h2>\n",
    "\n",
    "Up to now, we looked at how single cell activity varies across different conditions, like which image was shown, or whether the mouse was running or not. \n",
    "But are these conditions reliable predictors of cell activity? \n",
    "\n",
    "To say that a cell \"encodes\" something, we want to know that the cell activity is reliably predictable based on that something. \n",
    "In other words, can we model a cell's activity based on different variables or predictors?\n",
    "\n",
    "Regression models provide a mathematical framework for investigating these questions. Here, we will use linear regression to investigate which behavioral features are encoded by neurons.\n",
    "\n",
    "\n",
    "Questions: \n",
    "    \n",
    "(1) How can linear regression be used to model neural coding? \n",
    "\n",
    "(2) How do you ensure that your model is valid and is not overfitting?\n",
    "\n",
    "(3) How well can you predict neural activity based on stimulus information? Behavioral information? \n",
    "\n",
    "(4) Does the prediction improve when additional variables are included? \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "456c9318",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h3> Linear regression </h3>\n",
    "\n",
    "    \n",
    "In a regression problem, we are have pairs of data points  $(_,_)$\n",
    "  where  [1,]. We want to develop a function  $( )$\n",
    "  such that  $(_)_$\n",
    "  for each pair of points in the data set.\n",
    "    \n",
    "    \n",
    "The simplest regression problem is linear regression, in which we try to create the function $f$ by linearly combining a set of functions that act on the points $x$.\n",
    "\n",
    "$f(\\vec{x}_i) = \\sum_j w_j \\phi(\\vec{x}_i)$\n",
    "\n",
    "The functions $\\phi(\\vec{x})$ are chosen according to the question you are trying to answer. They are often called \"features\".  \n",
    "    \n",
    "The coefficients $w_j$ are called \"weights.\" When we talk about fitting a regression model, what we mean is determining the best set of weights for our  $(_) \\rightarrow _$ mapping? \n",
    "\n",
    "\n",
    "\n",
    "But what is the \"best\" set of weights? We try to choose the weights that minimize overall error between $f(x)$ and $y$.In the case of linear regression we use the sum of squared residuals between our for each $( )$ and the corresponding $y_i$:\n",
    "\n",
    "$E = \\frac{1}{2} \\sum_i \\left | y_i - f\\left ( \\vec{x}_i \\right ) \\right |^2 = \\frac{1}{2} \\sum_i \\left | y_i - \\sum_j w_j \\phi (\\vec{x}_i ) \\right |^2 $\n",
    "\n",
    "\n",
    "\n",
    "This particular problem has an exact analytic solution that is easy to implement, but in this tutorial, we will look at how to perform regression using the `scikit-learn` Python package.  `scikit-learn` has many regression algorithms in common use built in, most of which do not have simple analytic solutions.  In addition, other packages have adopted the `scikit-learn` style interface.  One advantage of this is that multiple algorithms can be deployed with the same code.\n",
    "\n",
    "\n",
    "\n",
    "The `scikit-learn` website:  http://scikit-learn.org/stable/\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a2b9754",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3> Question 1: How can linear regression be used to model neural coding? </h3>\n",
    "<p>\n",
    "\n",
    "You may be familiar with a version of linear regression where the functions   are chosen to be the identity and a constant. When the input space is one dimensional this is:\n",
    "\n",
    "$(_i)=\\sum_j w_j \\phi(\\vec{x}_i) = _i+c$\n",
    "\n",
    "This simple model assumes that $f(x)$ scales linearly as a function of $x$. \n",
    "\n",
    "However, even if the variables in our model do not have a perfect linear relationship, this model might still be useful; in practice, so long as $x$ and $y$ have a monotonic relationship, we would still expect to see the model explain some fraction of the variance in our data. This is equivalent to saying that $x$ and $y$ are linearly correlated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b52955e7",
   "metadata": {},
   "source": [
    "Above, we noted that some cells have a correlation with the animals pupil width during each stimulus. First, lets use linear regression to mathamatically formalize this relationship. \n",
    "\n",
    "Once again, we can use the `stimulus_response_df` we computed to get the mean pupil width and mean response for each stimulus presentation. We will use these to start fiting our model. This time around, however, we will our pupil width `X` and our mean response `y` to be consistent with the math we just saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To match the equations above and the sklearn convention,\n",
    "# We will call our predictor / encoded variable \"X\" and our response variable \"y\"\n",
    "X = cell_df.mean_pupil_width.values\n",
    "y = cell_df.mean_response.values\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, y, '.')\n",
    "ax.set_xlabel('Mean pupil width')\n",
    "ax.set_ylabel('Mean cell response')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51cc5f80",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "If $x$ and $y$ are correlated, we will fit a non-zero value for $w$ when we do our model fit. However, simply noting the correlation would be *Descriptive* - it wouldn't tell us anything about how consistent this correlation is across the dataset. A few spurious data points could lead to a correlation that does not hold throughout our data. \n",
    "\n",
    "`scipy` has pre-implemented calculators for data correlation. Here, we can quickly test if, for example, our data are pearson correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208742f",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use `pearsonr` to compute the correlation between the mean cell response and the mean pupil width on each stimulus presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation of X and y (pupil width and cell activity)\n",
    "pearson_corr,pearson_pval = pearsonr(X, y)\n",
    "pearson_corr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "236aadee",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Regression provides a *Predictive* model. For new values of $x$, we can produce an estimate of what $y$ should be. Importantly, the predictive nature of our model also proves to be an important tool for assessing whether our model consistently represents our data. \n",
    "\n",
    "We do this by splitting our dataset into parts. Just as we did above, we will train the model on on part of our dataset, then evaluate it on data that was withheld from this initial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(4)# Setting the random seed here insures that everyone gets the same result when they run this notebook!\n",
    "\n",
    "# Use sklearn train_test_split \n",
    "y_train, y_test, X_train, X_test = train_test_split(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do each of these splits look like? \n",
    "print('length of y_train', len(y_train))\n",
    "print('length of y_test', len(y_test))\n",
    "print('length of X_train', len(X_train))\n",
    "print('length of X_test', len(X_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cef20fdb",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Now that we have our data ready, we can import the `scikit-learn` package (we will call it \"sklearn\" to save some typing). It has a nice interface for fitting regression models that allows us to not worry about implementing our own solution for the cost function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d12225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158d7c6",
   "metadata": {},
   "source": [
    "The sklearn interface is object oriented. This means that to fit a model, we need to instantiate a \"LinearRegression\" object. We will then use this for to handle our fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2942b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LinearRegression object\n",
    "LR = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e834bf",
   "metadata": {},
   "source": [
    "Now that we have our object, we can fit data using the built in \"fit\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de4e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that LinearRegression requires X to be two dimensional - why this is will be apparent shortly\n",
    "LR.fit(X_train.reshape(-1, 1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdeb26e",
   "metadata": {},
   "source": [
    "<!-- Thats it! We have our first model!  -->\n",
    "\n",
    "We can now look carefully at the `sklearn` object to learn about the model fit we just performed. Here, `coef_` contains the weight vector $\\vec{w}$ for our model, and `intercept_` contains the constant $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188de53",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Examine the `coef_` and `intercept_` attributes of the `LinearRegression` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coef_ attribute of the LinearRegression object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the intercept_ attribute of the LinearRegression object\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e54f7a67",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now that we know the weight and intercept for this model, plot the line we just fit, overlaid with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0057ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x and y values\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(X_train, y_train,'.')\n",
    "ax.plot(X_test, y_test,'.')\n",
    "# Plot the fit \n",
    "xx = [np.min(X),np.max(X)]\n",
    "ax.plot(xx,LR.coef_*xx+LR.intercept_)\n",
    "# Label the axes\n",
    "ax.set_xlabel('Mean pupil width')\n",
    "ax.set_ylabel('Mean cell response')\n",
    "ax.set_title('Corr. coeff. = {:.2}'.format(pearson_corr));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "613845a0",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Now let's look at how well our model does at predicting data.\n",
    "\n",
    "The `LinearRegression` object has a method to evaluate the `score` of the model. \n",
    "\n",
    "Here we will provide the training data for X and y (pupil and cell response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0097707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did we do with our training data.\n",
    "LR.score(X_train.reshape(-1,1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa175249",
   "metadata": {},
   "source": [
    "Is this number meaningful? We will delve into this now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b09972a0",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3> Question 2: How do you ensure that your model is valid and is not \"overfitting\"?</h3>\n",
    "<p>\n",
    "\n",
    "\"Overfitting\" is a term used to describe the case in which learns does very well in describing the data that it is trained on, but fails to predict new or additional data. Another way of saying this that a model will learn to describe noise or idiosyncrasies of the training data, rather than the underlying relationships that you are trying to model.\n",
    "\n",
    "To illustrate overfitting, lets pause for a quick thought experiment. Imagine that, instead of fitting the two parameter model we just used, we fit a model with $N$ parameters where $N$ is the number of data points. Our model, which could look like this:\n",
    "\n",
    "$(_i)=\\sum_{i = [1,N]} w_i\\vec{x}_i$\n",
    "\n",
    "We call this a \"saturated model\" because it is saturated with parameters. Once we fit this model, we would discover that we could now *perfectly* predict every single data point. In this linear case, we would now find $f(x_i)=y_i$, with error of 0. \n",
    "\n",
    "So...why don't we do this? Wasn't our goal to get the lowest error possible? \n",
    "\n",
    "You have probably already noticed the two big problems with this saturated model. First, we can't learn anything from its weights. Regression is guided dimensionality reduction exercise, where we try and describe our data with a chosen set of features. The saturated model fails to do this. Second, this model is worthless for explaining new data. It assumes a 1-to-1 $x_i\\rightarrow y_i$ mapping, and is undefined for new points or allows assumes no variance for repeated observations.\n",
    "\n",
    "Even if we move away from this extreme case, it is still possible to overfit a model simply by fitting weights to too many features. Spurious correlations in your data will appear to be explained by the additional features in your training data, only to limit your ability to predict held out data. \n",
    "\n",
    "<b>\n",
    "In this section, we will introduce a technique known as \"Cross-Validation\" as a way to systematically test if you have a \"good\" model.\n",
    "\n",
    "<p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34b7ebda",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "*One more aside about saturated models: the deliberately overfit saturated model is always going to be the model with the lower error possible error. It is therefore useful as as a comparison tool to determine how well your model fit does. In the linear case, the error of the saturated model is always 0 so we don't really need to think about it. If, however, we are doing non-linear regression, the saturated model becomes a useful in assessing model performance.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1973435a",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "What is the models performance on our held-out testing data (`X_test` and `y_test`?\n",
    "\n",
    "Use the `score` method again, but provide the held out test data instead.\n",
    "\n",
    "Is it lower or higher than the score for the training data? Is it greater than or less than zero? What can we learn from answering these questions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the score for the held out test data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48c3bba8",
   "metadata": {},
   "source": [
    "A score of 0.028 indicates that we explain 2.8% of the variance in our test data by using this model. This isn't a lot, and it is lower than the variance we explained using the training data suggesting that we aren't doing quite as well at predicting the pupil-neuron relationship in this new data. However, it is greater than 0, which means our model has SOME predictive power, even if it is small!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ef23b4f",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "This \"training\" and \"testing\" split approach would be great if data were always cheap and plentiful. In practice however, it can be frustrating to use most of your hard-earned data to train a model, only learn how it performs on a held out subset.\n",
    "\n",
    "A common aproach to dealing with this problem is known as <b>*Cross Validation*</b> Here, we systematically hold out chunks of data, refitting our model on the remaining data each time. By performing multiple model fits, we can (1) use all our data and (2) get a better sense of how our data varies across the dataset.\n",
    "\n",
    "There are many ways to do cross validation, and how you split things up can have a big influence on the question you are trying to answer. Lets start with one of the simplist and most common forms of cross validation, known as <b>KFolds</b>. Here, we split (i.e. fold) our data $k$ times, with equal sample sizes in each fold. We then fit $k$ models to our data. \n",
    "\n",
    "What is <b>$k$</b>? We will use 5 for now. It can be anywere from 2 to $n$ where $n$ is the number of samples in your dataset. This extreme case, where $n$ is the same as the number of samples you have is called \"leave-one-out\" cross validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ab9ce",
   "metadata": {},
   "source": [
    "Here is an example of how to do 5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc127b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 5 fold cross validation\n",
    "n_folds = 5\n",
    "fold_num = np.zeros(len(X))\n",
    "\n",
    "# Sort data into 5 groups:\n",
    "fold_group = np.arange(0,len(X))%n_folds\n",
    "fold_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e45d9",
   "metadata": {},
   "source": [
    "Now that we have groups, we can loop through and fit a model to each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test & training score for each fold\n",
    "\n",
    "# Create an empty array to save the data in\n",
    "self_score = np.empty(n_folds)\n",
    "cross_score = np.empty(n_folds)\n",
    "# Loop over folds, fit the model, and compute the scores on training & test data\n",
    "for ii in range(n_folds):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X[fold_group!=ii].reshape(-1,1), y[fold_group!=ii])\n",
    "    self_score[ii] = lr.score(X[fold_group!=ii].reshape(-1,1), y[fold_group!=ii])\n",
    "    cross_score[ii] = lr.score(X[fold_group==ii].reshape(-1,1), y[fold_group==ii])\n",
    "\n",
    "# Print out the results\n",
    "print(f'Training Score: {self_score}')\n",
    "print(f'Testing Score: {cross_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436b6d4",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "What is the average test score across folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of the scores across folds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b37b96da",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "`sklearn` provides a convient object for splitting data, so that you don't need to write your own splitting code. It is called `KFolds` and is housed in the `model_selection` modual.\n",
    "\n",
    "Just as we did with the `Regression` object, the `scikit-learn` interface has us instantiate a `KFold` object, which provides a generator object that we can use to loop through our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Generate the folds\n",
    "folderizer = KFold(n_splits=5)\n",
    "folderizer.split(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41ab96cc",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "What is a generator object? Each time it is called, it will generate the next of n folds in our data. It can therefore be incorporated into a for loop using the following syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149ace22",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Let's repeat the exercise above, to fit the model and compute the score across folds, but now using the `KFold` generator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KFold object\n",
    "n_folds = 5\n",
    "folderizer = KFold(n_splits=n_folds,)\n",
    "\n",
    "# Create an empty array to save the data in\n",
    "self_score = np.empty(n_folds)\n",
    "cross_score = np.empty(n_folds)\n",
    "# Loop through folds, fit model, save scores\n",
    "for ii, (train_index, test_index) in enumerate(folderizer.split(X, y)):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X[train_index].reshape(-1,1), y[train_index])\n",
    "    self_score[ii] = lr.score(X[train_index].reshape(-1,1), y[train_index])\n",
    "    cross_score[ii] = lr.score(X[test_index].reshape(-1,1), y[test_index])\n",
    "# Print out the results\n",
    "print(f'Training Score: {self_score}')\n",
    "print(f'Testing Score: {cross_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b1c6a",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Compute the average testing score across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6afc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean and print it out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "672a39e8",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Discussion questions:\n",
    "\n",
    "(1) Why is this different from what we did before? Hint: read the documentation on KFold so see how it splits the data.\n",
    "\n",
    "(2) What does this tell us about pupil responses in our data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1532405c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3>Question 3: How well can you predict neural activity based on stimulus information? Does the prediction improve when additional variables are included?</h3>\n",
    "\n",
    "One of the useful things about regression models is that that can be used evaluate the role of different kinds of features in predicting data. \n",
    "\n",
    "Earlier, we saw that examples of how stimulus identity can be encoded by the activity of a single neuron. Here, we will recast this tuning problem as a regression problem, allowing us to use our regression tool-box to understand this tuning. \n",
    "\n",
    "We will then see how using a common modeling framework allows us to quantitatively compare the encoding of different features by analyzing the contribution of each feature to explaining variance in neural activity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f217591",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3>Casting tuning as a regression problem</h3>\n",
    "    \n",
    "    \n",
    "Because our stimulus here consists of a set of 8 discrete images, we need to adapt this framework to make predictions based on predictions are based on a categorical rather than continuous variable (i.e., one of $8$ possible images). In other words, as above, we seek a model of the form:\n",
    "\n",
    "$$y = \\beta x+C,$$\n",
    "\n",
    "where $y$ is the calcium response, $X$ is now the stimulus identity (a catigorical variable), and $\\beta$ and $C$ are constants. \n",
    "\n",
    "One way to handle this would be to construct a separate model for each orientation:\n",
    "$$y = \\beta_1 X_1+C_1$$\n",
    "$$y = \\beta_2 X_2+C_2$$\n",
    "$$\\vdots$$\n",
    "$$y = \\beta_8 X_8+C_8 $$\n",
    "    \n",
    "Mathematically, this is cumbersome - we would need to look up which equation to use each time we want to predict new data. A more elegant alternative is to combine predictors across orientations into a single model that simply operates piecewise:\n",
    "\n",
    "$$y = C+ \\begin{cases} \n",
    "\\beta_1 X * \\text{I}_1(X)  \\\\\n",
    "\\beta_2 X * \\text{I}_2(X) \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_8 X * \\text{I}_8(X)\n",
    "\\end{cases}$$\n",
    "    \n",
    "where $\\text{I}_n(X)$ is the <i>indicator function</i>:\n",
    "$$ \\text{I}_n(X) := \\begin{cases}\n",
    "1 \\text{ if } X=n, \\\\\n",
    "0 \\text{ else}\n",
    "\\end{cases} $$\n",
    "\n",
    "(Notice that this formulation merges the constants into one value, $C$. $C$ is, effectivly, the offset from zero for any model we fit.)\n",
    "\n",
    "Thus, as $X$ encodes the stimulus identity, $\\text{I}_n(X)$ determines which term in the equation we are operating with. This type of problem is called *\"One-Hot\"* encoding, because $X$ encodes what part of the equation is active. Practically speaking, we can implement this indicator function by creating a vector for each sample and setting $X_i = 1$ for whichever case is true. For example, if we had just two stimulus types, we might have: \n",
    "\n",
    "$$ X_1 = [1,0] $$\n",
    "$$ X_2 = [0,1] $$ \n",
    "\n",
    "Finally, if we have many observations, we can stack each of these $X$ observations to form a \"Design Matrix.\" \n",
    "\n",
    "We will have a corresponding fitting parameter vector, $$\\beta = [\\beta_1,\\beta_2,\\ldots,\\beta_8]$$\n",
    "\n",
    "Our whole problem can now be written: \n",
    "$$y = \\beta X$$ \n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5d08197",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Let's build out our X matrix (the features we are using to predict our cell activity), as described above, using the stimulus identity for each stimulus presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98685a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the X matrix, which is the image identity presented on each trial, encoded as a one-hot vector\n",
    "\n",
    "# Get index for each image for each stimulus presentation \n",
    "img_index = cell_df.image_index.values\n",
    "# Create an array the length of stimulus presentations by 8 (the number of images)\n",
    "X = np.zeros((len(img_index),8))\n",
    "# Loop through image indices and build up the X matrix \n",
    "for ii in range(len(img_index)):\n",
    "    X[ii,img_index[ii]]  = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61c00637",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now plot it. Label the axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the X matrix \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec18a22",
   "metadata": {},
   "source": [
    "Looks pretty gross, right? X is too tall a matrix to be easily visualized. Lets zoom in on the first bit of it to get a sense for whats really going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is more intuitive if we zoom in\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(X[:20]) \n",
    "ax.set_ylabel('Stimulus presentation #')\n",
    "ax.set_xlabel('Image #')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f9f2995",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Now that we have our design matrix, X, using using it to fit a model is quite simple. We use the same `LinearRegression` object as before, but now fit with our new design matrix.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model with the LinearRegression object and our new design matrix X, with the cell activity y\n",
    "model = LinearRegression(fit_intercept=False).fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7b1880c",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now plot the model coefficients, which are the predictions of the cell's response for each image. \n",
    "\n",
    "How does this compare to the turning curve we plotted earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coef_ attribute. Label the axes. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bee2a1e",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "You will have notice that we used a new setting when we created the `LinearRegression` object, `fit_intercept=False`. This prevents `LinearRegression` from fitting the constant/intercept term in our model. \n",
    "\n",
    "If we were to include this term, the model fit would be ill posed. Our model performance would be the same, but our data would be shifted by an aribtary constant. To see this, try fitting a model with `fit_intercept=True` and looking `coef_` and `intercept_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d998f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with fit_intercept = True and look at the coefficients and intercept\n",
    "funky_model = LinearRegression(fit_intercept=True).fit(X, y)\n",
    "print(f'Coefs: {funky_model.coef_}')\n",
    "print(f'Intercept: {funky_model.intercept_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the intercept is just a shift in the data, we can add it back to the\n",
    "# coefs to recover our origional model\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(funky_model.coef_+funky_model.intercept_)\n",
    "ax.set_xlabel('image index')\n",
    "ax.set_ylabel('Model Coefficient')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47dd7371",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now that we know how to use our design matrix, we properly evaluate it using Kfold Cross validation. \n",
    "\n",
    "This is done exactly as we did above, with the addition of our new regression parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7754ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set the seed so you get the same result here no matter what order you run this notebook in!\n",
    "np.random.seed(5) \n",
    "\n",
    "# Initialize KFold object\n",
    "folderizer = KFold(n_splits=5,shuffle=True)\n",
    "\n",
    "# Create arrays to save the results\n",
    "self_score = np.empty(n_folds)\n",
    "cross_score = np.empty(n_folds)\n",
    "models = [None]*5\n",
    "\n",
    "# Loop over folds, fit the model and collect the scores\n",
    "for ii, (train_index,test_index) in enumerate(folderizer.split(X,y)):\n",
    "    models[ii] = LinearRegression(fit_intercept=False).fit(X[train_index,:], y[train_index])\n",
    "    self_score[ii] = models[ii].score(X[train_index,:], y[train_index])\n",
    "    cross_score[ii] = models[ii].score(X[test_index,:], y[test_index])\n",
    "print(f'Training Score: {self_score}')\n",
    "print(f'Testing Score: {cross_score}')\n",
    "print(f'Average Testing Score: {np.mean(cross_score)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04e419",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Loop through the folds, as we did above, but now plot the coefficients for each fold to check how consistent the results are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot axis to visualize the results\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Loop over folds and plot the coefficients\n",
    "\n",
    "\n",
    "# Label the axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97597a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d10c4eb",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3> How do stimulus and behavior information compare? </h3>\n",
    "    \n",
    "By fitting models with different combinations of features, we can get a richer sense of how different features are encoded by neural activity. \n",
    "\n",
    "One option for doing this is simply to fit a model to each variable of interest and compare their performance. This answers a very simple question: how much of a cells variability can be explained by this particular feature. We will see, however, that when variables are correlated the outcome of such one-at-a-time model fits can be difficult to interpret. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73cfaee2",
   "metadata": {},
   "source": [
    "\n",
    "This next step is going to involve a bunch of model fitting, using the same basic procedure we outlined in the previous section. Before we go on, lets take a quick momement to move our KFold Linear Model fitting into a function so we don't have to type so much!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75222e4d",
   "metadata": {},
   "source": [
    "Note that the `KFold` object includes both `shuffle` and `shuffle_seed` parameters. `shuffle` does exactly what it sounds like- it randomizes the set data points included in each fold. `shuffle_seed` can be used to get reproducible results from this shuffling. This is particularly important if we want to compare models- using the same shuffle seed will give the same random set of trials across function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ee571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidateLinearModel(X, y , n_split = 5, shuffle = False, shuffle_seed = None):\n",
    "    '''\n",
    "    Cross validate a linear model using KFold cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input data to fit\n",
    "    y : np.array\n",
    "        The output data to fit\n",
    "    n_split : int\n",
    "        The number of splits to use\n",
    "    shuffle : bool\n",
    "        Whether or not to shuffle the data\n",
    "    shuffle_seed : int\n",
    "        The seed to use for shuffling the data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Mean Score: float\n",
    "        The average cross validation score\n",
    "    Model List: list    \n",
    "        The models fit to each fold of the data\n",
    "    Test score: np.array    \n",
    "        The cross validation scores for testing data each fold\n",
    "    Train score: np.array\n",
    "        The cross validation scores for testing data each fold\n",
    "    '''\n",
    "\n",
    "    if len(X.shape)==1:\n",
    "        X = X.copy().reshape(-1,1)\n",
    "    # Initialize KFold object\n",
    "    folderizer = KFold(n_splits=n_split,shuffle=shuffle,random_state=shuffle_seed)\n",
    "    # Create an array to save the results\n",
    "    self_score = np.empty(n_folds)\n",
    "    cross_score = np.empty(n_folds)\n",
    "    models = [None]*n_split\n",
    "    # Loop through the folds, fit the model, and save the results\n",
    "    for ii, (train_index, test_index) in enumerate(folderizer.split(X,y)):\n",
    "        models[ii] = LinearRegression(fit_intercept=False).fit(X[train_index,:], y[train_index])\n",
    "        self_score[ii] = models[ii].score(X[train_index,:], y[train_index])\n",
    "        cross_score[ii] = models[ii].score(X[test_index,:], y[test_index])\n",
    "        \n",
    "    return np.mean(cross_score),models,cross_score,self_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "481af6ce",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Create an X matrix for each variable we want to test: stimulus, pupil, and running. \n",
    "\n",
    "Use the `mean_pupil_width`, `mean_running_speed` and `mean_response` columns of the `stimulus_response_df` for our cell. \n",
    "\n",
    "Remember that we previously we saved this data as `cell_df` above.\n",
    "\n",
    "Print out the shapes of each X matrix. \n",
    "\n",
    "Plot the X matrices for tunning and pupil to see what they look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create design matrix for each feature\n",
    "X_stim = X.copy()\n",
    "X_pupil = cell_df.mean_pupil_width.values\n",
    "X_running = cell_df.mean_running_speed.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162117b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are their shapes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the running and pupil design matrices to see what they look like\n",
    "# Bonus if you use a twinx() to see both on the same axis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea00ecf",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use the function we created above to cross-validate and test our linear model for each of the variables. \n",
    "\n",
    "Which one produces the highest score? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d729e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use our fancy new function to test a bunch of models.\n",
    "seed = 5\n",
    "x_stim_score,_,_,_  = crossValidateLinearModel(X_stim, y, shuffle = True, shuffle_seed=seed)\n",
    "print(f'Stimulus model score {x_stim_score}')\n",
    "\n",
    "x_pupil_score,_,_,_  = crossValidateLinearModel(X_pupil, y, shuffle = True, shuffle_seed=seed)\n",
    "print(f'Pupil model score {x_pupil_score}')\n",
    "\n",
    "x_running_score,_,_,_  = crossValidateLinearModel(X_running, y, shuffle = True, shuffle_seed=seed)\n",
    "print(f'Running model score {x_running_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc6b449f",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h4> What if our variables are correlated? </h4>\n",
    "\n",
    "At face value it looks like our cell of choice heavily encodes stimulus identify, with weaker encoding of running and pupil size.\n",
    "\n",
    "So...can we go to an early lunch? Not quite. The challence here is that running and pupil diameter are not necessarily indpendent variables. This makes that fact that our neuron shows a weak correlation with both of them difficult to interpret.\n",
    "\n",
    "Let's plot the relationship of running and pupil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37fb8179",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now, lets return to the relationship between <b>pupil width</b> and <b>running</b> , using our design matrices which contain the average value of these variables for each stimulus presentation. \n",
    "\n",
    "Compute the <b>pearson correlation</b>  and put it in the title of the plot. Is the relationship significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pupil vs running. Label your axes.\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_pupil, X_running)\n",
    "ax.set_xlabel('Mean pupil')\n",
    "ax.set_ylabel('Mean running')\n",
    "\n",
    "# Compute the correlation and print it out\n",
    "r, p = pearsonr(X_pupil, X_running)\n",
    "ax.set_title('r = '+str(np.round(r, 3)))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19f439",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3> Question 4: Does the prediction improve when additional variables are included? </h3>\n",
    "\n",
    "In regression models, we are not limited to considering each feature one-at-a time. Instead, multiple features or sets of features can be combined into a single model simply by combining them in the design matrix used to train that model.\n",
    "\n",
    "This is particularly useful when trying to determine if correlated features are uniquely encoded by a cell. To close today, we will see two methods for asking whether or not a given feature explains some of cell's variance beyond what could have been explained by our other modeled features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3a6c4",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3> Multiple linear regression </h3>\n",
    "\n",
    "*Multiple Linear Regression* gives us tools to dissect the contributions of different features in explaining variance.\n",
    "\n",
    "Just as we built a design matrix out of different stimulus identities, we can similary construct one that includes additional features about our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6877f5",
   "metadata": {},
   "source": [
    "Let's create an X matrix that incorporates stimulus, pupil, and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack up the x matrices to make one big feature matrix\n",
    "X_combo = np.hstack((X_stim, X_pupil.reshape(-1,1), X_running.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced3eb5",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use our `crossValidateLinearModel` function from before to get the model prediction for this multi-variate X matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide X_combo and y to the function we defined above\n",
    "x_combo_score,_,_,_  = crossValidateLinearModel(X_combo, y, shuffle=True, shuffle_seed=seed)\n",
    "print(f'Combined model score {x_combo_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4f2dabf",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "Importantly, we cannot simply look at the model coefficients, as we did in the \"stimulus only\" example. This is because our model now contains different types of features with different magnitudes, and there is not a clear mapping between them. While the weights we fit will scale accordingly, they can no longer be directly compared. Visualizing the design matrix illustrates this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc51fa8",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Plot the design matrix `X_combo` for the first 20 stimulus presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.imshow(X_combo[:20]) \n",
    "ax.set_ylabel('Trial #')\n",
    "ax.set_xlabel('Image #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f12e8",
   "metadata": {},
   "source": [
    "This shows images 1-8, then running speed for each trial, then pupil width for each trial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61418a19",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Leave one out test </h4>\n",
    "\n",
    "Instead, we can use the model scores - that is, the variance in our data explained by our model - to test the encoding of any particular feature.\n",
    "\n",
    "Specifically, we can systematically drop out feature one at a time and see how model performance changes. If the model gets worse, it suggests that this feature was explaining some of the variance in our data. Because other features are still included, this method is a way to avoid mistakenly assuming that a cell encodes all of a set of correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe9ee9",
   "metadata": {},
   "source": [
    "Let's create several design matrices, each with one of the variables left out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c71cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create each design matrix as a stacked combo of all the features except the one we are leaving out\n",
    "X_wout_stimulus = np.hstack((X_pupil.reshape(-1,1), X_running.reshape(-1,1)))\n",
    "X_wout_running =  np.hstack((X_stim, X_pupil.reshape(-1,1)))\n",
    "X_wout_pupil =  np.hstack((X_stim, X_running.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02920f96",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Use the `crossValidateLinearModel` function again to test model performance on each of the leave one out design matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d216d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use our fancy new function to test a bunch of models.\n",
    "x_combo_score,_,_,_  = crossValidateLinearModel(X_combo, y, shuffle = True, shuffle_seed=seed)\n",
    "print(f'Combo {x_combo_score}')\n",
    "\n",
    "x_wout_stim_score,_,_,_  = crossValidateLinearModel(X_wout_stimulus, y, shuffle = True, shuffle_seed=seed)\n",
    "print(f'Wout Stim {x_wout_stim_score}')\n",
    "print(f'Additional variance explained by stim {x_combo_score-x_wout_stim_score}')\n",
    "\n",
    "x_wout_pupil_score,_,_,_  = crossValidateLinearModel(X_wout_pupil, y, shuffle = True, shuffle_seed=seed)\n",
    "print(f'Wout Pupil {x_wout_pupil_score}')\n",
    "print(f'Additional variance explained by pupil {x_combo_score-x_wout_pupil_score}')\n",
    "\n",
    "x_wout_running_score,_,_,_  = crossValidateLinearModel(X_wout_running, y, shuffle = True, shuffle_seed=seed)\n",
    "print(f'Wout_Running {x_wout_running_score}')\n",
    "print(f'Additional variance explained by running {x_combo_score-x_wout_running_score}');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32e43430",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "<h4> Impact of behavioral choice on model prediction </h4>\n",
    "\n",
    "In part 2 above, we also suggested that cells encoded the difference between <b>hit</b> and <b>miss</b> trials (image changes with or without a lick + reward). \n",
    "\n",
    "Build a complete model that includes these features using the same one-hot encoding method we used for the stimulus, then use this to quanitfy the relative contribution of stimulus identity vs. choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad04e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values from the stimulus response df for our cell\n",
    "X_hit = cell_df.hit.values\n",
    "X_lick = cell_df.licked.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the full model\n",
    "X_full = np.hstack((X_stim, X_pupil.reshape(-1,1), X_running.reshape(-1,1), X_hit.reshape(-1,1), X_lick.reshape(-1,1)))\n",
    "\n",
    "# And subset models lacking each variable\n",
    "X_wout_hit = np.hstack((X_stim, X_pupil.reshape(-1,1), X_running.reshape(-1,1), X_lick.reshape(-1,1)))\n",
    "X_wout_lick = np.hstack((X_stim, X_pupil.reshape(-1,1), X_running.reshape(-1,1), X_hit.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95030558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "x_full_score,_,_,_  = crossValidateLinearModel(X_full, y, shuffle = True,shuffle_seed=seed)\n",
    "print(f'Combo {x_full_score}')\n",
    "\n",
    "x_wout_hit_score,_,_,_  = crossValidateLinearModel(X_wout_hit, y, shuffle = True,shuffle_seed=seed)\n",
    "print(f'Wout hits {x_wout_hit_score}')\n",
    "print(f'Additional variance explained by hit-vs-miss {x_full_score-x_wout_hit_score}')\n",
    "\n",
    "x_wout_lick_score,_,_,_  = crossValidateLinearModel(X_wout_lick, y, shuffle = True, shuffle_seed=seed)\n",
    "print(f'Wout licks {x_wout_lick_score}')\n",
    "print(f'Additional variance explained by licking {x_full_score-x_wout_lick_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "189a9ea5",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "This result may be surpising, given that previously we showed that tunning can be different on hits vs misses.\n",
    "\n",
    "To understand why this is happening, we need to think about what question we are asking with our model. In the previous problem, we used a single feature to encode whether a trial was a hit or a miss. This allows for changes in overal response on hit vs miss trials, but cannot account more subtil difference in tuning.\n",
    "\n",
    "There are a couple ways to answer this question. One would be to adapt the linear modeling framework to handle two conditions of stimuli: hits vs misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e92ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which trials are hits and misses\n",
    "is_hit = X_hit.astype(bool)\n",
    "\n",
    "# Build two copies of the stimulus matrix: one for hits and one for misses\n",
    "# When a trial is not in that category, set all values to 0.\n",
    "X_hit_stim = X_stim.copy()\n",
    "X_hit_stim[~is_hit,:] = 0\n",
    "X_miss_stim = X_stim.copy()\n",
    "X_miss_stim[is_hit,:] = 0\n",
    "\n",
    "# Combine everything into a single design matrix.\n",
    "X_seperate_stim = np.hstack((X_hit_stim, X_miss_stim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6d92b",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec279168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LinearRegression object with X_separate_stim matrix and the y value (cell responses) to predict\n",
    "lr = LinearRegression(fit_intercept=False).fit(X_seperate_stim,y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68966052",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Now when we will, effectively, get two sets of coefficient: one for hit trials and one for miss trials. They will be combined into a single coefficient vector. \n",
    "\n",
    "Plot each set of coefficients; do you see differences? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ebfa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coefficients for the hits and misses separately\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(lr.coef_[:8],label= 'Hits')\n",
    "ax.plot(lr.coef_[8:],label = 'Misses')\n",
    "ax.set_xlabel('Image index')\n",
    "ax.set_ylabel('Model coef')\n",
    "ax.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b5c9cca",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "In this case, simply using random cross validation would be insufficient. This is because we are somewhat data-starved in this problem: There are not equal numbers of hits and misses, and they are not randomly distributed throughout the session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824b5fe",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Look at the pattern of hits and misses across trials. How many are there of each? When do they happen during the session? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8607fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot X_hit over time to see how many trials are hits vs misses\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aee6d9e3",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "For now, we can return to using the `train_test_split` function to split our data in half. Due to the nature of our questions, however, we are going to get to try out a few extra features of this function.\n",
    "\n",
    "First, We will `stratify` our data using the 'hit' variable. This will ensure that both halves of the data have equal numbers of hits and misses.\n",
    "\n",
    "Next, we will force the training and test sets to be of equal size, each containing half of our data.\n",
    "\n",
    "We will call this function twice, once to split the design matrix with our trials separated, once for the un-separated data. Because we want to get the same trial splits between these two conditions, we will set `random_state` to be the same between these function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e67de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random_state to be the same here ensure that we grab  the same halves of both design matricies.\n",
    "y_train, y_test, X_sep_train, X_sep_test = train_test_split(y, X_seperate_stim, stratify=X_hit, test_size=.5, train_size=.5, random_state=0)\n",
    "_, _, X_nosep_train, X_nosep_test = train_test_split(y, X_stim, stratify=X_hit,test_size=.5, train_size=.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53142530",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "Fit two models with the training data we just pulled out: one using separated stimuli and the other using un-separated stimuli. Evaluate these models using the test data. How much of a difference does splitting hits and miss stimuli make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sep = LinearRegression(fit_intercept=False).fit(X_sep_train, y_train)\n",
    "print(f'Mean variance explained with hits and misses seperated {lr_sep.score(X_sep_test, y_test)}')\n",
    "lr_nosep = LinearRegression(fit_intercept=False).fit(X_nosep_train, y_train)\n",
    "print(f'Mean variance explained with without seperation {lr_nosep.score(X_nosep_test, y_test)}')\n",
    "print(f'Improvment from seperation {lr_sep.score(X_sep_test, y_test)-lr_nosep.score(X_nosep_test, y_test)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcad0373",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "\n",
    "The other way to aproach this problem would be using the predictive nature of the linear model. We can train a model on one kind of data (e.g misses) and ask how well it does at explaining misses. Because there are far more \"miss\" trials, lets use split these to train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc499",
   "metadata": {},
   "source": [
    "We can start by seperating 'hit' and 'miss' trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stim_miss = X_stim[~is_hit,:]\n",
    "y_miss = y[~is_hit]\n",
    "X_stim_hit = X_stim[is_hit,:]\n",
    "y_hit = y[is_hit]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1beff5f",
   "metadata": {},
   "source": [
    "Now, lets build a model using only miss trials. \n",
    "\n",
    "We still need to separate the miss trials in training and testing data, so that we can compare our model performance on left out miss trials to its performance on hit trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split miss trials\n",
    "y_miss_train, y_miss_test, X_stim_miss_train, X_stim_miss_test = train_test_split(y_miss, X_stim_miss, test_size=.5, train_size=.5, random_state=0)\n",
    "# Fit model\n",
    "lr_miss = LinearRegression(fit_intercept=False).fit(X_stim_miss_train, y_miss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62b14f",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>\n",
    "\n",
    "We can now compare the performance of a 'miss' trial model in predicting both held-out miss trials and hit trials. \n",
    "\n",
    "Compute the score for the miss model with the test data with misses, compared to the hit trials. Is a miss trial model any good at predicting activity durring hit trials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on misses test data\n",
    "lr_miss.score(X_stim_miss_test, y_miss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on hits\n",
    "lr_miss.score(X_stim_hit, y_hit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dfcd89c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3> Question 5: Going nonlinear [BONUS MATERIAL] </h3>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5881a1fd",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "So far se have focused on linear models of the form \n",
    "\n",
    "$f(X) = \\beta X = \\beta_0X_0 + \\beta_1X_1+...+\\beta_nXn$\n",
    "\n",
    "This is commonly refered to as a \"General Linear Model,\" or gLM. These are super useful, but this model class makes a big assumption about the variance distribution of our data: specifically, we assumed that variance is normally distributed about some mean value, $f(x)$.\n",
    "\n",
    "We can, however, arbitrarly modify this variance assumption. To achieve this this, we a non-linearly, $g(*)$, to our equation:\n",
    "\n",
    "$f(X) = g(\\beta X)$\n",
    "\n",
    "or equlivalently:\n",
    "\n",
    "$g(f(X))^{-1}= \\beta X$\n",
    "\n",
    "This later case is what you will see in literature more often, where $g(*)^{-1}$ is commonly called the 'link' function. \n",
    "\n",
    "In principle, $g(*)$ can be whatever we want. We choose $g(*)^{-1}$ that match the variance model we want to capture. Adding the nonlinearity, however, ruins the closed-form solution for $\\beta$ that we elluded to in the strictly linear case. Instead, we need to use an optimizer to solve for $\\beta$ in this new non-linear case. Such solvers work by maximizing the likelyhood (or equivalently minimizign the negative log-likelihood) of potential $\\beta$ values for a given dataset. If $g(*)$ describes variance models from the exponential family of distributions, it can be shown that convergence is garanteed for this optimization problem.\n",
    "\n",
    "The class of models created by including $g(*)$ is commonly refered to as \"General*ized* Linear Models,\" or GLMs. Yes, this nomenclature is confusing- if it makes you feel any better, the people who created it later admitted they should have chosen a better name. You will sometimes also hear of these models refered to as \"Linear-Non-linear\" models, because the linear and nonlinear parts can be mathematically seperated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc13f9",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Why do we bring up these these non-linearities? \n",
    "    \n",
    "There is a specific case that frequently arrising in neuroscience where the assumption of normally distributed variance is particularly bad: *spiketrains*.\n",
    "\n",
    "Spikes are discrete, transient events. Over a given time interval, there can never be fewer than 0 spikes, and there can never be a fractional number of spikes. If we assume that spikes are occure indepently with an average rate (call it $\\lambda$), then a poisson distribution (https://en.wikipedia.org/wiki/Poisson_distribution) can be shown to be a good model of this variance. To give our GLM poisson varinace, we use the log link function $g(\\lambda)^{-1}=ln(\\lambda)$.\n",
    "\n",
    "Now, our goal with our GLM will be to predict what the mean rate, so we need to fit a model of the fit our new model:\n",
    "\n",
    "$ln(\\lambda) = \\beta X$\n",
    "    \n",
    "Fortunatly, `sklearn` ships with an implementation of poisson regression, as well as many other commonly used link functions (e.g. those for exponential data, logistic data, etc. https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models). \n",
    "    \n",
    "In practice, this means that switching to a Poisson GLM requires relativly minimal changes to our code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970cf41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "PLM = PoissonRegressor(fit_intercept=False)\n",
    "PLM.fit(X_stim,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c031c2",
   "metadata": {},
   "source": [
    "Right now we aren't working with spikes! The sklearn package won't accept non discrete data as input, since poisson regression assumes that data must be integer values. \n",
    "\n",
    "In the next workshop you will delve more deeply into spiking data. Later, as an excersize, come back to this section and try some of the regression techniques we saw above with spiketrains and poisson regression. This generalized cross validation function should help get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859093ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidateGeneralModel(X,y,model =LinearRegression,n_split = 5,shuffle = False,shuffle_seed = None):\n",
    "    '''\n",
    "    Super informative docstring goes here! \n",
    "    Should writing it be an excersize?\n",
    "    '''\n",
    "    if len(X.shape)==1:\n",
    "        X = X.copy().reshape(-1,1)\n",
    "    folderizer = KFold(n_splits=n_split,shuffle=shuffle,random_state=shuffle_seed)\n",
    "    self_score = np.empty(n_folds)\n",
    "    cross_score = np.empty(n_folds)\n",
    "    models = [None]*n_split\n",
    "    for ii, (train_index,test_index) in enumerate(folderizer.split(X,y)):\n",
    "        models[ii] = model(fit_intercept=False).fit(X[train_index,:],y[train_index])\n",
    "        self_score[ii] = models[ii].score(X[train_index,:],y[train_index])\n",
    "        cross_score[ii] = models[ii].score(X[test_index,:],y[test_index])\n",
    "    return np.mean(cross_score),models,cross_score,self_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f37fc",
   "metadata": {},
   "source": [
    "Note that the `PoissonRegressor.score` function behaves slightly differently from the linear regression version. It reterns \"Deviance explained,\" rather than \"Variance Explained.\" The difference are subtil, but means that the two numbers cannot be directly compared. Read more about these metrics here https://scikit-learn.org/stable/modules/model_evaluation.html#d2-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df2698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b43922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1ee00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d68067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041b91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddbafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1de02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e91d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 66.770791,
   "end_time": "2023-07-31T19:18:26.861555",
   "environment_variables": {},
   "exception": null,
   "input_path": "doc_template/examples_root/examples/nb/visual_behavior_load_ophys_data.ipynb",
   "output_path": "/tmp/tmpcq7t4kmr/scratch_nb.ipynb",
   "parameters": {
    "output_dir": "/tmp/tmpcq7t4kmr",
    "resources_dir": "/home/runner/work/AllenSDK/AllenSDK/allensdk/internal/notebooks/resources"
   },
   "start_time": "2023-07-31T19:17:20.090764",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
